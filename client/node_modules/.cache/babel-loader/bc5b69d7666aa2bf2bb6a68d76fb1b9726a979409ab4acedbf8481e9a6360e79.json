{"ast":null,"code":"import _objectWithoutProperties from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/objectWithoutProperties.js\";\nimport _toConsumableArray from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _objectSpread from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\nimport _asyncToGenerator from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _regeneratorRuntime from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _createClass from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _classCallCheck from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _inherits from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\nimport _wrapNativeSuper from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/wrapNativeSuper.js\";\nimport _asyncIterator from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/asyncIterator.js\";\nimport _awaitAsyncGenerator from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/awaitAsyncGenerator.js\";\nimport _wrapAsyncGenerator from \"/Users/ayzo/cs178/project/showup/showup/showup/client/node_modules/@babel/runtime/helpers/esm/wrapAsyncGenerator.js\";\nvar _excluded = [\"onMessage\", \"onError\"];\n// src/chatgpt-api.ts\nimport Keyv from \"keyv\";\nimport pTimeout from \"p-timeout\";\nimport QuickLRU from \"quick-lru\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n// src/tokenizer.ts\nimport { get_encoding } from \"@dqbd/tiktoken\";\nvar tokenizer = get_encoding(\"cl100k_base\");\nfunction encode(input) {\n  return tokenizer.encode(input);\n}\n\n// src/types.ts\nvar ChatGPTError = /*#__PURE__*/function (_Error) {\n  _inherits(ChatGPTError, _Error);\n  var _super = _createSuper(ChatGPTError);\n  function ChatGPTError() {\n    _classCallCheck(this, ChatGPTError);\n    return _super.apply(this, arguments);\n  }\n  return _createClass(ChatGPTError);\n}( /*#__PURE__*/_wrapNativeSuper(Error));\nvar openai;\n(function (openai2) {})(openai || (openai = {}));\n\n// src/fetch.ts\nvar fetch = globalThis.fetch;\n\n// src/fetch-sse.ts\nimport { createParser } from \"eventsource-parser\";\n\n// src/stream-async-iterable.ts\nfunction streamAsyncIterable(_x) {\n  return _streamAsyncIterable.apply(this, arguments);\n} // src/fetch-sse.ts\nfunction _streamAsyncIterable() {\n  _streamAsyncIterable = _wrapAsyncGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(stream) {\n    var reader, _yield$_awaitAsyncGen, done, value;\n    return _regeneratorRuntime().wrap(function _callee$(_context) {\n      while (1) switch (_context.prev = _context.next) {\n        case 0:\n          reader = stream.getReader();\n          _context.prev = 1;\n        case 2:\n          if (!true) {\n            _context.next = 14;\n            break;\n          }\n          _context.next = 5;\n          return _awaitAsyncGenerator(reader.read());\n        case 5:\n          _yield$_awaitAsyncGen = _context.sent;\n          done = _yield$_awaitAsyncGen.done;\n          value = _yield$_awaitAsyncGen.value;\n          if (!done) {\n            _context.next = 10;\n            break;\n          }\n          return _context.abrupt(\"return\");\n        case 10:\n          _context.next = 12;\n          return value;\n        case 12:\n          _context.next = 2;\n          break;\n        case 14:\n          _context.prev = 14;\n          reader.releaseLock();\n          return _context.finish(14);\n        case 17:\n        case \"end\":\n          return _context.stop();\n      }\n    }, _callee, null, [[1,, 14, 17]]);\n  }));\n  return _streamAsyncIterable.apply(this, arguments);\n}\nfunction fetchSSE(_x2, _x3) {\n  return _fetchSSE.apply(this, arguments);\n} // src/chatgpt-api.ts\nfunction _fetchSSE() {\n  _fetchSSE = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee10(url, options) {\n    var fetch2,\n      onMessage,\n      onError,\n      fetchOptions,\n      res,\n      reason,\n      msg,\n      error,\n      parser,\n      feed,\n      body,\n      _iteratorAbruptCompletion,\n      _didIteratorError,\n      _iteratorError,\n      _iterator,\n      _step,\n      chunk,\n      str,\n      _args10 = arguments;\n    return _regeneratorRuntime().wrap(function _callee10$(_context10) {\n      while (1) switch (_context10.prev = _context10.next) {\n        case 0:\n          fetch2 = _args10.length > 2 && _args10[2] !== undefined ? _args10[2] : fetch;\n          onMessage = options.onMessage, onError = options.onError, fetchOptions = _objectWithoutProperties(options, _excluded);\n          _context10.next = 4;\n          return fetch2(url, fetchOptions);\n        case 4:\n          res = _context10.sent;\n          if (res.ok) {\n            _context10.next = 20;\n            break;\n          }\n          _context10.prev = 6;\n          _context10.next = 9;\n          return res.text();\n        case 9:\n          reason = _context10.sent;\n          _context10.next = 15;\n          break;\n        case 12:\n          _context10.prev = 12;\n          _context10.t0 = _context10[\"catch\"](6);\n          reason = res.statusText;\n        case 15:\n          msg = \"ChatGPT error \".concat(res.status, \": \").concat(reason);\n          error = new ChatGPTError(msg, {\n            cause: res\n          });\n          error.statusCode = res.status;\n          error.statusText = res.statusText;\n          throw error;\n        case 20:\n          parser = createParser(function (event) {\n            if (event.type === \"event\") {\n              onMessage(event.data);\n            }\n          });\n          feed = function feed(chunk) {\n            var _a;\n            var response = null;\n            try {\n              response = JSON.parse(chunk);\n            } catch (_unused) {}\n            if (((_a = response == null ? void 0 : response.detail) == null ? void 0 : _a.type) === \"invalid_request_error\") {\n              var _msg = \"ChatGPT error \".concat(response.detail.message, \": \").concat(response.detail.code, \" (\").concat(response.detail.type, \")\");\n              var _error = new ChatGPTError(_msg, {\n                cause: response\n              });\n              _error.statusCode = response.detail.code;\n              _error.statusText = response.detail.message;\n              if (onError) {\n                onError(_error);\n              } else {\n                console.error(_error);\n              }\n              return;\n            }\n            parser.feed(chunk);\n          };\n          if (res.body.getReader) {\n            _context10.next = 29;\n            break;\n          }\n          body = res.body;\n          if (!(!body.on || !body.read)) {\n            _context10.next = 26;\n            break;\n          }\n          throw new ChatGPTError('unsupported \"fetch\" implementation');\n        case 26:\n          body.on(\"readable\", function () {\n            var chunk;\n            while (null !== (chunk = body.read())) {\n              feed(chunk.toString());\n            }\n          });\n          _context10.next = 58;\n          break;\n        case 29:\n          _iteratorAbruptCompletion = false;\n          _didIteratorError = false;\n          _context10.prev = 31;\n          _iterator = _asyncIterator(streamAsyncIterable(res.body));\n        case 33:\n          _context10.next = 35;\n          return _iterator.next();\n        case 35:\n          if (!(_iteratorAbruptCompletion = !(_step = _context10.sent).done)) {\n            _context10.next = 42;\n            break;\n          }\n          chunk = _step.value;\n          str = new TextDecoder().decode(chunk);\n          feed(str);\n        case 39:\n          _iteratorAbruptCompletion = false;\n          _context10.next = 33;\n          break;\n        case 42:\n          _context10.next = 48;\n          break;\n        case 44:\n          _context10.prev = 44;\n          _context10.t1 = _context10[\"catch\"](31);\n          _didIteratorError = true;\n          _iteratorError = _context10.t1;\n        case 48:\n          _context10.prev = 48;\n          _context10.prev = 49;\n          if (!(_iteratorAbruptCompletion && _iterator.return != null)) {\n            _context10.next = 53;\n            break;\n          }\n          _context10.next = 53;\n          return _iterator.return();\n        case 53:\n          _context10.prev = 53;\n          if (!_didIteratorError) {\n            _context10.next = 56;\n            break;\n          }\n          throw _iteratorError;\n        case 56:\n          return _context10.finish(53);\n        case 57:\n          return _context10.finish(48);\n        case 58:\n        case \"end\":\n          return _context10.stop();\n      }\n    }, _callee10, null, [[6, 12], [31, 44, 48, 58], [49,, 53, 57]]);\n  }));\n  return _fetchSSE.apply(this, arguments);\n}\nvar CHATGPT_MODEL = \"gpt-3.5-turbo\";\nvar USER_LABEL_DEFAULT = \"User\";\nvar ASSISTANT_LABEL_DEFAULT = \"ChatGPT\";\nvar ChatGPTAPI = /*#__PURE__*/function () {\n  /**\n   * Creates a new client wrapper around OpenAI's chat completion API, mimicing the official ChatGPT webapp's functionality as closely as possible.\n   *\n   * @param apiKey - OpenAI API key (required).\n   * @param apiOrg - Optional OpenAI API organization (optional).\n   * @param apiBaseUrl - Optional override for the OpenAI API base URL.\n   * @param debug - Optional enables logging debugging info to stdout.\n   * @param completionParams - Param overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   * @param maxModelTokens - Optional override for the maximum number of tokens allowed by the model's context. Defaults to 4096.\n   * @param maxResponseTokens - Optional override for the minimum number of tokens allowed for the model's response. Defaults to 1000.\n   * @param messageStore - Optional [Keyv](https://github.com/jaredwray/keyv) store to persist chat messages to. If not provided, messages will be lost when the process exits.\n   * @param getMessageById - Optional function to retrieve a message by its ID. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param upsertMessage - Optional function to insert or update a message. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  function ChatGPTAPI(opts) {\n    _classCallCheck(this, ChatGPTAPI);\n    var apiKey = opts.apiKey,\n      apiOrg = opts.apiOrg,\n      _opts$apiBaseUrl = opts.apiBaseUrl,\n      apiBaseUrl = _opts$apiBaseUrl === void 0 ? \"https://api.openai.com/v1\" : _opts$apiBaseUrl,\n      _opts$debug = opts.debug,\n      debug = _opts$debug === void 0 ? false : _opts$debug,\n      messageStore = opts.messageStore,\n      completionParams = opts.completionParams,\n      systemMessage = opts.systemMessage,\n      _opts$maxModelTokens = opts.maxModelTokens,\n      maxModelTokens = _opts$maxModelTokens === void 0 ? 4e3 : _opts$maxModelTokens,\n      _opts$maxResponseToke = opts.maxResponseTokens,\n      maxResponseTokens = _opts$maxResponseToke === void 0 ? 1e3 : _opts$maxResponseToke,\n      getMessageById = opts.getMessageById,\n      upsertMessage = opts.upsertMessage,\n      _opts$fetch = opts.fetch,\n      fetch2 = _opts$fetch === void 0 ? fetch : _opts$fetch;\n    this._apiKey = apiKey;\n    this._apiOrg = apiOrg;\n    this._apiBaseUrl = apiBaseUrl;\n    this._debug = !!debug;\n    this._fetch = fetch2;\n    this._completionParams = _objectSpread({\n      model: CHATGPT_MODEL,\n      temperature: 0.8,\n      top_p: 1,\n      presence_penalty: 1\n    }, completionParams);\n    this._systemMessage = systemMessage;\n    if (this._systemMessage === void 0) {\n      var currentDate = /* @__PURE__ */new Date().toISOString().split(\"T\")[0];\n      this._systemMessage = \"You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\\nKnowledge cutoff: 2021-09-01\\nCurrent date: \".concat(currentDate);\n    }\n    this._maxModelTokens = maxModelTokens;\n    this._maxResponseTokens = maxResponseTokens;\n    this._getMessageById = getMessageById !== null && getMessageById !== void 0 ? getMessageById : this._defaultGetMessageById;\n    this._upsertMessage = upsertMessage !== null && upsertMessage !== void 0 ? upsertMessage : this._defaultUpsertMessage;\n    if (messageStore) {\n      this._messageStore = messageStore;\n    } else {\n      this._messageStore = new Keyv({\n        store: new QuickLRU({\n          maxSize: 1e4\n        })\n      });\n    }\n    if (!this._apiKey) {\n      throw new Error(\"OpenAI missing required apiKey\");\n    }\n    if (!this._fetch) {\n      throw new Error(\"Invalid environment; fetch is not defined\");\n    }\n    if (typeof this._fetch !== \"function\") {\n      throw new Error('Invalid \"fetch\" is not a function');\n    }\n  }\n  /**\n   * Sends a message to the OpenAI chat completions endpoint, waits for the response\n   * to resolve, and returns the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI chat completions API. You can override the `systemMessage` in `opts` to customize the assistant's instructions.\n   *\n   * @param message - The prompt message to send\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.conversationId - Optional ID of the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.systemMessage - Optional override for the chat \"system message\" which acts as instructions to the model (defaults to the ChatGPT system message)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   * @param completionParams - Optional overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   *\n   * @returns The response from ChatGPT\n   */\n  _createClass(ChatGPTAPI, [{\n    key: \"sendMessage\",\n    value: function () {\n      var _sendMessage = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(text) {\n        var _this = this;\n        var opts,\n          parentMessageId,\n          _opts$messageId,\n          messageId,\n          timeoutMs,\n          onProgress,\n          _opts$stream,\n          stream,\n          completionParams,\n          conversationId,\n          abortSignal,\n          abortController,\n          message,\n          latestQuestion,\n          _yield$this$_buildMes,\n          messages,\n          maxTokens,\n          numTokens,\n          result,\n          responseP,\n          _args4 = arguments;\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              opts = _args4.length > 1 && _args4[1] !== undefined ? _args4[1] : {};\n              parentMessageId = opts.parentMessageId, _opts$messageId = opts.messageId, messageId = _opts$messageId === void 0 ? uuidv4() : _opts$messageId, timeoutMs = opts.timeoutMs, onProgress = opts.onProgress, _opts$stream = opts.stream, stream = _opts$stream === void 0 ? onProgress ? true : false : _opts$stream, completionParams = opts.completionParams, conversationId = opts.conversationId;\n              abortSignal = opts.abortSignal;\n              abortController = null;\n              if (timeoutMs && !abortSignal) {\n                abortController = new AbortController();\n                abortSignal = abortController.signal;\n              }\n              message = {\n                role: \"user\",\n                id: messageId,\n                conversationId: conversationId,\n                parentMessageId: parentMessageId,\n                text: text\n              };\n              latestQuestion = message;\n              _context4.next = 9;\n              return this._buildMessages(text, opts);\n            case 9:\n              _yield$this$_buildMes = _context4.sent;\n              messages = _yield$this$_buildMes.messages;\n              maxTokens = _yield$this$_buildMes.maxTokens;\n              numTokens = _yield$this$_buildMes.numTokens;\n              result = {\n                role: \"assistant\",\n                id: uuidv4(),\n                conversationId: conversationId,\n                parentMessageId: messageId,\n                text: \"\"\n              };\n              responseP = new Promise( /*#__PURE__*/function () {\n                var _ref = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(resolve, reject) {\n                  var _a, _b, url, headers, body, res, reason, msg, error, response, message2, res2;\n                  return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n                    while (1) switch (_context2.prev = _context2.next) {\n                      case 0:\n                        url = \"\".concat(_this._apiBaseUrl, \"/chat/completions\");\n                        headers = {\n                          \"Content-Type\": \"application/json\",\n                          Authorization: \"Bearer \".concat(_this._apiKey)\n                        };\n                        body = _objectSpread(_objectSpread(_objectSpread({\n                          max_tokens: maxTokens\n                        }, _this._completionParams), completionParams), {}, {\n                          messages: messages,\n                          stream: stream\n                        });\n                        if (_this._apiOrg) {\n                          headers[\"OpenAI-Organization\"] = _this._apiOrg;\n                        }\n                        if (_this._debug) {\n                          console.log(\"sendMessage (\".concat(numTokens, \" tokens)\"), body);\n                        }\n                        if (!stream) {\n                          _context2.next = 9;\n                          break;\n                        }\n                        fetchSSE(url, {\n                          method: \"POST\",\n                          headers: headers,\n                          body: JSON.stringify(body),\n                          signal: abortSignal,\n                          onMessage: function onMessage(data) {\n                            var _a2;\n                            if (data === \"[DONE]\") {\n                              result.text = result.text.trim();\n                              return resolve(result);\n                            }\n                            try {\n                              var response = JSON.parse(data);\n                              if (response.id) {\n                                result.id = response.id;\n                              }\n                              if ((_a2 = response.choices) == null ? void 0 : _a2.length) {\n                                var delta = response.choices[0].delta;\n                                result.delta = delta.content;\n                                if (delta == null ? void 0 : delta.content) result.text += delta.content;\n                                if (delta.role) {\n                                  result.role = delta.role;\n                                }\n                                result.detail = response;\n                                onProgress == null ? void 0 : onProgress(result);\n                              }\n                            } catch (err) {\n                              console.warn(\"OpenAI stream SEE event unexpected error\", err);\n                              return reject(err);\n                            }\n                          }\n                        }, _this._fetch).catch(reject);\n                        _context2.next = 42;\n                        break;\n                      case 9:\n                        _context2.prev = 9;\n                        _context2.next = 12;\n                        return _this._fetch(url, {\n                          method: \"POST\",\n                          headers: headers,\n                          body: JSON.stringify(body),\n                          signal: abortSignal\n                        });\n                      case 12:\n                        res = _context2.sent;\n                        if (res.ok) {\n                          _context2.next = 22;\n                          break;\n                        }\n                        _context2.next = 16;\n                        return res.text();\n                      case 16:\n                        reason = _context2.sent;\n                        msg = \"OpenAI error \".concat(res.status || res.statusText, \": \").concat(reason);\n                        error = new ChatGPTError(msg, {\n                          cause: res\n                        });\n                        error.statusCode = res.status;\n                        error.statusText = res.statusText;\n                        return _context2.abrupt(\"return\", reject(error));\n                      case 22:\n                        _context2.next = 24;\n                        return res.json();\n                      case 24:\n                        response = _context2.sent;\n                        if (_this._debug) {\n                          console.log(response);\n                        }\n                        if (response == null ? void 0 : response.id) {\n                          result.id = response.id;\n                        }\n                        if (!((_a = response == null ? void 0 : response.choices) == null ? void 0 : _a.length)) {\n                          _context2.next = 33;\n                          break;\n                        }\n                        message2 = response.choices[0].message;\n                        result.text = message2.content;\n                        if (message2.role) {\n                          result.role = message2.role;\n                        }\n                        _context2.next = 35;\n                        break;\n                      case 33:\n                        res2 = response;\n                        return _context2.abrupt(\"return\", reject(new Error(\"OpenAI error: \".concat(((_b = res2 == null ? void 0 : res2.detail) == null ? void 0 : _b.message) || (res2 == null ? void 0 : res2.detail) || \"unknown\"))));\n                      case 35:\n                        result.detail = response;\n                        return _context2.abrupt(\"return\", resolve(result));\n                      case 39:\n                        _context2.prev = 39;\n                        _context2.t0 = _context2[\"catch\"](9);\n                        return _context2.abrupt(\"return\", reject(_context2.t0));\n                      case 42:\n                      case \"end\":\n                        return _context2.stop();\n                    }\n                  }, _callee2, null, [[9, 39]]);\n                }));\n                return function (_x5, _x6) {\n                  return _ref.apply(this, arguments);\n                };\n              }()).then( /*#__PURE__*/function () {\n                var _ref2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(message2) {\n                  var promptTokens, completionTokens;\n                  return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n                    while (1) switch (_context3.prev = _context3.next) {\n                      case 0:\n                        if (!(message2.detail && !message2.detail.usage)) {\n                          _context3.next = 11;\n                          break;\n                        }\n                        _context3.prev = 1;\n                        promptTokens = numTokens;\n                        _context3.next = 5;\n                        return _this._getTokenCount(message2.text);\n                      case 5:\n                        completionTokens = _context3.sent;\n                        message2.detail.usage = {\n                          prompt_tokens: promptTokens,\n                          completion_tokens: completionTokens,\n                          total_tokens: promptTokens + completionTokens,\n                          estimated: true\n                        };\n                        _context3.next = 11;\n                        break;\n                      case 9:\n                        _context3.prev = 9;\n                        _context3.t0 = _context3[\"catch\"](1);\n                      case 11:\n                        return _context3.abrupt(\"return\", Promise.all([_this._upsertMessage(latestQuestion), _this._upsertMessage(message2)]).then(function () {\n                          return message2;\n                        }));\n                      case 12:\n                      case \"end\":\n                        return _context3.stop();\n                    }\n                  }, _callee3, null, [[1, 9]]);\n                }));\n                return function (_x7) {\n                  return _ref2.apply(this, arguments);\n                };\n              }());\n              if (!timeoutMs) {\n                _context4.next = 20;\n                break;\n              }\n              if (abortController) {\n                ;\n                responseP.cancel = function () {\n                  abortController.abort();\n                };\n              }\n              return _context4.abrupt(\"return\", pTimeout(responseP, {\n                milliseconds: timeoutMs,\n                message: \"OpenAI timed out waiting for response\"\n              }));\n            case 20:\n              return _context4.abrupt(\"return\", responseP);\n            case 21:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function sendMessage(_x4) {\n        return _sendMessage.apply(this, arguments);\n      }\n      return sendMessage;\n    }()\n  }, {\n    key: \"apiKey\",\n    get: function get() {\n      return this._apiKey;\n    },\n    set: function set(apiKey) {\n      this._apiKey = apiKey;\n    }\n  }, {\n    key: \"apiOrg\",\n    get: function get() {\n      return this._apiOrg;\n    },\n    set: function set(apiOrg) {\n      this._apiOrg = apiOrg;\n    }\n  }, {\n    key: \"_buildMessages\",\n    value: function () {\n      var _buildMessages2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee5(text, opts) {\n        var _opts$systemMessage, systemMessage, parentMessageId, userLabel, assistantLabel, maxNumTokens, messages, systemMessageOffset, nextMessages, numTokens, prompt, nextNumTokensEstimate, isValidPrompt, parentMessage, parentMessageRole, maxTokens;\n        return _regeneratorRuntime().wrap(function _callee5$(_context5) {\n          while (1) switch (_context5.prev = _context5.next) {\n            case 0:\n              _opts$systemMessage = opts.systemMessage, systemMessage = _opts$systemMessage === void 0 ? this._systemMessage : _opts$systemMessage;\n              parentMessageId = opts.parentMessageId;\n              userLabel = USER_LABEL_DEFAULT;\n              assistantLabel = ASSISTANT_LABEL_DEFAULT;\n              maxNumTokens = this._maxModelTokens - this._maxResponseTokens;\n              messages = [];\n              if (systemMessage) {\n                messages.push({\n                  role: \"system\",\n                  content: systemMessage\n                });\n              }\n              systemMessageOffset = messages.length;\n              nextMessages = text ? messages.concat([{\n                role: \"user\",\n                content: text,\n                name: opts.name\n              }]) : messages;\n              numTokens = 0;\n            case 10:\n              prompt = nextMessages.reduce(function (prompt2, message) {\n                switch (message.role) {\n                  case \"system\":\n                    return prompt2.concat([\"Instructions:\\n\".concat(message.content)]);\n                  case \"user\":\n                    return prompt2.concat([\"\".concat(userLabel, \":\\n\").concat(message.content)]);\n                  default:\n                    return prompt2.concat([\"\".concat(assistantLabel, \":\\n\").concat(message.content)]);\n                }\n              }, []).join(\"\\n\\n\");\n              _context5.next = 13;\n              return this._getTokenCount(prompt);\n            case 13:\n              nextNumTokensEstimate = _context5.sent;\n              isValidPrompt = nextNumTokensEstimate <= maxNumTokens;\n              if (!(prompt && !isValidPrompt)) {\n                _context5.next = 17;\n                break;\n              }\n              return _context5.abrupt(\"break\", 32);\n            case 17:\n              messages = nextMessages;\n              numTokens = nextNumTokensEstimate;\n              if (isValidPrompt) {\n                _context5.next = 21;\n                break;\n              }\n              return _context5.abrupt(\"break\", 32);\n            case 21:\n              if (parentMessageId) {\n                _context5.next = 23;\n                break;\n              }\n              return _context5.abrupt(\"break\", 32);\n            case 23:\n              _context5.next = 25;\n              return this._getMessageById(parentMessageId);\n            case 25:\n              parentMessage = _context5.sent;\n              if (parentMessage) {\n                _context5.next = 28;\n                break;\n              }\n              return _context5.abrupt(\"break\", 32);\n            case 28:\n              parentMessageRole = parentMessage.role || \"user\";\n              nextMessages = nextMessages.slice(0, systemMessageOffset).concat([{\n                role: parentMessageRole,\n                content: parentMessage.text,\n                name: parentMessage.name\n              }].concat(_toConsumableArray(nextMessages.slice(systemMessageOffset))));\n              parentMessageId = parentMessage.parentMessageId;\n            case 31:\n              if (true) {\n                _context5.next = 10;\n                break;\n              }\n            case 32:\n              maxTokens = Math.max(1, Math.min(this._maxModelTokens - numTokens, this._maxResponseTokens));\n              return _context5.abrupt(\"return\", {\n                messages: messages,\n                maxTokens: maxTokens,\n                numTokens: numTokens\n              });\n            case 34:\n            case \"end\":\n              return _context5.stop();\n          }\n        }, _callee5, this);\n      }));\n      function _buildMessages(_x8, _x9) {\n        return _buildMessages2.apply(this, arguments);\n      }\n      return _buildMessages;\n    }()\n  }, {\n    key: \"_getTokenCount\",\n    value: function () {\n      var _getTokenCount2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee6(text) {\n        return _regeneratorRuntime().wrap(function _callee6$(_context6) {\n          while (1) switch (_context6.prev = _context6.next) {\n            case 0:\n              text = text.replace(/<\\|endoftext\\|>/g, \"\");\n              return _context6.abrupt(\"return\", encode(text).length);\n            case 2:\n            case \"end\":\n              return _context6.stop();\n          }\n        }, _callee6);\n      }));\n      function _getTokenCount(_x10) {\n        return _getTokenCount2.apply(this, arguments);\n      }\n      return _getTokenCount;\n    }()\n  }, {\n    key: \"_defaultGetMessageById\",\n    value: function () {\n      var _defaultGetMessageById2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee7(id) {\n        var res;\n        return _regeneratorRuntime().wrap(function _callee7$(_context7) {\n          while (1) switch (_context7.prev = _context7.next) {\n            case 0:\n              _context7.next = 2;\n              return this._messageStore.get(id);\n            case 2:\n              res = _context7.sent;\n              return _context7.abrupt(\"return\", res);\n            case 4:\n            case \"end\":\n              return _context7.stop();\n          }\n        }, _callee7, this);\n      }));\n      function _defaultGetMessageById(_x11) {\n        return _defaultGetMessageById2.apply(this, arguments);\n      }\n      return _defaultGetMessageById;\n    }()\n  }, {\n    key: \"_defaultUpsertMessage\",\n    value: function () {\n      var _defaultUpsertMessage2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee8(message) {\n        return _regeneratorRuntime().wrap(function _callee8$(_context8) {\n          while (1) switch (_context8.prev = _context8.next) {\n            case 0:\n              _context8.next = 2;\n              return this._messageStore.set(message.id, message);\n            case 2:\n            case \"end\":\n              return _context8.stop();\n          }\n        }, _callee8, this);\n      }));\n      function _defaultUpsertMessage(_x12) {\n        return _defaultUpsertMessage2.apply(this, arguments);\n      }\n      return _defaultUpsertMessage;\n    }()\n  }]);\n  return ChatGPTAPI;\n}();\n\n// src/chatgpt-unofficial-proxy-api.ts\nimport pTimeout2 from \"p-timeout\";\nimport { v4 as uuidv42 } from \"uuid\";\n\n// src/utils.ts\nvar uuidv4Re = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i;\nfunction isValidUUIDv4(str) {\n  return str && uuidv4Re.test(str);\n}\n\n// src/chatgpt-unofficial-proxy-api.ts\nvar ChatGPTUnofficialProxyAPI = /*#__PURE__*/function () {\n  /**\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  function ChatGPTUnofficialProxyAPI(opts) {\n    _classCallCheck(this, ChatGPTUnofficialProxyAPI);\n    var accessToken = opts.accessToken,\n      _opts$apiReverseProxy = opts.apiReverseProxyUrl,\n      apiReverseProxyUrl = _opts$apiReverseProxy === void 0 ? \"https://bypass.duti.tech/api/conversation\" : _opts$apiReverseProxy,\n      _opts$model = opts.model,\n      model = _opts$model === void 0 ? \"text-davinci-002-render-sha\" : _opts$model,\n      _opts$debug2 = opts.debug,\n      debug = _opts$debug2 === void 0 ? false : _opts$debug2,\n      headers = opts.headers,\n      _opts$fetch2 = opts.fetch,\n      fetch2 = _opts$fetch2 === void 0 ? fetch : _opts$fetch2;\n    this._accessToken = accessToken;\n    this._apiReverseProxyUrl = apiReverseProxyUrl;\n    this._debug = !!debug;\n    this._model = model;\n    this._fetch = fetch2;\n    this._headers = headers;\n    if (!this._accessToken) {\n      throw new Error(\"ChatGPT invalid accessToken\");\n    }\n    if (!this._fetch) {\n      throw new Error(\"Invalid environment; fetch is not defined\");\n    }\n    if (typeof this._fetch !== \"function\") {\n      throw new Error('Invalid \"fetch\" is not a function');\n    }\n  }\n  _createClass(ChatGPTUnofficialProxyAPI, [{\n    key: \"accessToken\",\n    get: function get() {\n      return this._accessToken;\n    },\n    set: function set(value) {\n      this._accessToken = value;\n    }\n    /**\n     * Sends a message to ChatGPT, waits for the response to resolve, and returns\n     * the response.\n     *\n     * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n     *\n     * If you want to receive a stream of partial responses, use `opts.onProgress`.\n     * If you want to receive the full response, including message and conversation IDs,\n     * you can use `opts.onConversationResponse` or use the `ChatGPTAPI.getConversation`\n     * helper.\n     *\n     * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI completions API. You can override the `promptPrefix` and `promptSuffix` in `opts` to customize the prompt.\n     *\n     * @param message - The prompt message to send\n     * @param opts.conversationId - Optional ID of a conversation to continue (defaults to a random UUID)\n     * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n     * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n     * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n     * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n     * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n     *\n     * @returns The response from ChatGPT\n     */\n  }, {\n    key: \"sendMessage\",\n    value: function () {\n      var _sendMessage2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee9(text) {\n        var _this2 = this;\n        var opts,\n          conversationId,\n          _opts$parentMessageId,\n          parentMessageId,\n          _opts$messageId2,\n          messageId,\n          _opts$action,\n          action,\n          timeoutMs,\n          onProgress,\n          abortSignal,\n          abortController,\n          body,\n          result,\n          responseP,\n          _args9 = arguments;\n        return _regeneratorRuntime().wrap(function _callee9$(_context9) {\n          while (1) switch (_context9.prev = _context9.next) {\n            case 0:\n              opts = _args9.length > 1 && _args9[1] !== undefined ? _args9[1] : {};\n              if (!(!!opts.conversationId !== !!opts.parentMessageId)) {\n                _context9.next = 3;\n                break;\n              }\n              throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: conversationId and parentMessageId must both be set or both be undefined\");\n            case 3:\n              if (!(opts.conversationId && !isValidUUIDv4(opts.conversationId))) {\n                _context9.next = 5;\n                break;\n              }\n              throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: conversationId is not a valid v4 UUID\");\n            case 5:\n              if (!(opts.parentMessageId && !isValidUUIDv4(opts.parentMessageId))) {\n                _context9.next = 7;\n                break;\n              }\n              throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: parentMessageId is not a valid v4 UUID\");\n            case 7:\n              if (!(opts.messageId && !isValidUUIDv4(opts.messageId))) {\n                _context9.next = 9;\n                break;\n              }\n              throw new Error(\"ChatGPTUnofficialProxyAPI.sendMessage: messageId is not a valid v4 UUID\");\n            case 9:\n              conversationId = opts.conversationId, _opts$parentMessageId = opts.parentMessageId, parentMessageId = _opts$parentMessageId === void 0 ? uuidv42() : _opts$parentMessageId, _opts$messageId2 = opts.messageId, messageId = _opts$messageId2 === void 0 ? uuidv42() : _opts$messageId2, _opts$action = opts.action, action = _opts$action === void 0 ? \"next\" : _opts$action, timeoutMs = opts.timeoutMs, onProgress = opts.onProgress;\n              abortSignal = opts.abortSignal;\n              abortController = null;\n              if (timeoutMs && !abortSignal) {\n                abortController = new AbortController();\n                abortSignal = abortController.signal;\n              }\n              body = {\n                action: action,\n                messages: [{\n                  id: messageId,\n                  role: \"user\",\n                  content: {\n                    content_type: \"text\",\n                    parts: [text]\n                  }\n                }],\n                model: this._model,\n                parent_message_id: parentMessageId\n              };\n              if (conversationId) {\n                body.conversation_id = conversationId;\n              }\n              result = {\n                role: \"assistant\",\n                id: uuidv42(),\n                parentMessageId: messageId,\n                conversationId: conversationId,\n                text: \"\"\n              };\n              responseP = new Promise(function (resolve, reject) {\n                var url = _this2._apiReverseProxyUrl;\n                var headers = _objectSpread(_objectSpread({}, _this2._headers), {}, {\n                  Authorization: \"Bearer \".concat(_this2._accessToken),\n                  Accept: \"text/event-stream\",\n                  \"Content-Type\": \"application/json\"\n                });\n                if (_this2._debug) {\n                  console.log(\"POST\", url, {\n                    body: body,\n                    headers: headers\n                  });\n                }\n                fetchSSE(url, {\n                  method: \"POST\",\n                  headers: headers,\n                  body: JSON.stringify(body),\n                  signal: abortSignal,\n                  onMessage: function onMessage(data) {\n                    var _a, _b, _c;\n                    if (data === \"[DONE]\") {\n                      return resolve(result);\n                    }\n                    try {\n                      var convoResponseEvent = JSON.parse(data);\n                      if (convoResponseEvent.conversation_id) {\n                        result.conversationId = convoResponseEvent.conversation_id;\n                      }\n                      if ((_a = convoResponseEvent.message) == null ? void 0 : _a.id) {\n                        result.id = convoResponseEvent.message.id;\n                      }\n                      var message = convoResponseEvent.message;\n                      if (message) {\n                        var text2 = (_c = (_b = message == null ? void 0 : message.content) == null ? void 0 : _b.parts) == null ? void 0 : _c[0];\n                        if (text2) {\n                          result.text = text2;\n                          if (onProgress) {\n                            onProgress(result);\n                          }\n                        }\n                      }\n                    } catch (err) {\n                      if (_this2._debug) {\n                        console.warn(\"chatgpt unexpected JSON error\", err);\n                      }\n                    }\n                  },\n                  onError: function onError(err) {\n                    reject(err);\n                  }\n                }, _this2._fetch).catch(function (err) {\n                  var errMessageL = err.toString().toLowerCase();\n                  if (result.text && (errMessageL === \"error: typeerror: terminated\" || errMessageL === \"typeerror: terminated\")) {\n                    return resolve(result);\n                  } else {\n                    return reject(err);\n                  }\n                });\n              });\n              if (!timeoutMs) {\n                _context9.next = 22;\n                break;\n              }\n              if (abortController) {\n                ;\n                responseP.cancel = function () {\n                  abortController.abort();\n                };\n              }\n              return _context9.abrupt(\"return\", pTimeout2(responseP, {\n                milliseconds: timeoutMs,\n                message: \"ChatGPT timed out waiting for response\"\n              }));\n            case 22:\n              return _context9.abrupt(\"return\", responseP);\n            case 23:\n            case \"end\":\n              return _context9.stop();\n          }\n        }, _callee9, this);\n      }));\n      function sendMessage(_x13) {\n        return _sendMessage2.apply(this, arguments);\n      }\n      return sendMessage;\n    }()\n  }]);\n  return ChatGPTUnofficialProxyAPI;\n}();\nexport { ChatGPTAPI, ChatGPTError, ChatGPTUnofficialProxyAPI, openai };","map":{"version":3,"names":["Keyv","pTimeout","QuickLRU","v4","uuidv4","get_encoding","tokenizer","encode","input","ChatGPTError","_Error","_inherits","_super","_createSuper","_classCallCheck","apply","arguments","_createClass","_wrapNativeSuper","Error","openai","openai2","fetch","globalThis","createParser","streamAsyncIterable","_x","_streamAsyncIterable","_callee","stream","reader","_yield$_awaitAsyncGen","done","value","_regeneratorRuntime","wrap","_callee$","_context","prev","next","getReader","_awaitAsyncGenerator","read","sent","abrupt","releaseLock","finish","stop","fetchSSE","_x2","_x3","_fetchSSE","_callee10","url","options","fetch2","onMessage","onError","fetchOptions","res","reason","msg","error","parser","feed","body","_iteratorAbruptCompletion","_didIteratorError","_iteratorError","_iterator","_step","chunk","str","_args10","_callee10$","_context10","length","undefined","_objectWithoutProperties","_excluded","ok","text","t0","statusText","concat","status","cause","statusCode","event","type","data","_a","response","JSON","parse","_unused","detail","message","code","console","on","toString","_asyncIterator","TextDecoder","decode","t1","return","CHATGPT_MODEL","USER_LABEL_DEFAULT","ASSISTANT_LABEL_DEFAULT","ChatGPTAPI","opts","apiKey","apiOrg","_opts$apiBaseUrl","apiBaseUrl","_opts$debug","debug","messageStore","completionParams","systemMessage","_opts$maxModelTokens","maxModelTokens","_opts$maxResponseToke","maxResponseTokens","getMessageById","upsertMessage","_opts$fetch","_apiKey","_apiOrg","_apiBaseUrl","_debug","_fetch","_completionParams","_objectSpread","model","temperature","top_p","presence_penalty","_systemMessage","currentDate","Date","toISOString","split","_maxModelTokens","_maxResponseTokens","_getMessageById","_defaultGetMessageById","_upsertMessage","_defaultUpsertMessage","_messageStore","store","maxSize","key","_sendMessage","_asyncToGenerator","mark","_callee4","_this","parentMessageId","_opts$messageId","messageId","timeoutMs","onProgress","_opts$stream","conversationId","abortSignal","abortController","latestQuestion","_yield$this$_buildMes","messages","maxTokens","numTokens","result","responseP","_args4","_callee4$","_context4","AbortController","signal","role","id","_buildMessages","Promise","_ref","_callee2","resolve","reject","_b","headers","message2","res2","_callee2$","_context2","Authorization","max_tokens","log","method","stringify","_a2","trim","choices","delta","content","err","warn","catch","json","_x5","_x6","then","_ref2","_callee3","promptTokens","completionTokens","_callee3$","_context3","usage","_getTokenCount","prompt_tokens","completion_tokens","total_tokens","estimated","all","_x7","cancel","abort","milliseconds","sendMessage","_x4","get","set","_buildMessages2","_callee5","_opts$systemMessage","userLabel","assistantLabel","maxNumTokens","systemMessageOffset","nextMessages","prompt","nextNumTokensEstimate","isValidPrompt","parentMessage","parentMessageRole","_callee5$","_context5","push","name","reduce","prompt2","join","slice","_toConsumableArray","Math","max","min","_x8","_x9","_getTokenCount2","_callee6","_callee6$","_context6","replace","_x10","_defaultGetMessageById2","_callee7","_callee7$","_context7","_x11","_defaultUpsertMessage2","_callee8","_callee8$","_context8","_x12","pTimeout2","uuidv42","uuidv4Re","isValidUUIDv4","test","ChatGPTUnofficialProxyAPI","accessToken","_opts$apiReverseProxy","apiReverseProxyUrl","_opts$model","_opts$debug2","_opts$fetch2","_accessToken","_apiReverseProxyUrl","_model","_headers","_sendMessage2","_callee9","_this2","_opts$parentMessageId","_opts$messageId2","_opts$action","action","_args9","_callee9$","_context9","content_type","parts","parent_message_id","conversation_id","Accept","_c","convoResponseEvent","text2","errMessageL","toLowerCase","_x13"],"sources":["/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/chatgpt-api.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/tokenizer.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/types.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/fetch.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/fetch-sse.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/stream-async-iterable.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/chatgpt-unofficial-proxy-api.ts","/Users/ayzo/cs178/project/showup/showup/showup/node_modules/chatgpt/src/utils.ts"],"sourcesContent":["import Keyv from 'keyv'\nimport pTimeout from 'p-timeout'\nimport QuickLRU from 'quick-lru'\nimport { v4 as uuidv4 } from 'uuid'\n\nimport * as tokenizer from './tokenizer'\nimport * as types from './types'\nimport { fetch as globalFetch } from './fetch'\nimport { fetchSSE } from './fetch-sse'\n\nconst CHATGPT_MODEL = 'gpt-3.5-turbo'\n\nconst USER_LABEL_DEFAULT = 'User'\nconst ASSISTANT_LABEL_DEFAULT = 'ChatGPT'\n\nexport class ChatGPTAPI {\n  protected _apiKey: string\n  protected _apiBaseUrl: string\n  protected _apiOrg?: string\n  protected _debug: boolean\n\n  protected _systemMessage: string\n  protected _completionParams: Omit<\n    types.openai.CreateChatCompletionRequest,\n    'messages' | 'n'\n  >\n  protected _maxModelTokens: number\n  protected _maxResponseTokens: number\n  protected _fetch: types.FetchFn\n\n  protected _getMessageById: types.GetMessageByIdFunction\n  protected _upsertMessage: types.UpsertMessageFunction\n\n  protected _messageStore: Keyv<types.ChatMessage>\n\n  /**\n   * Creates a new client wrapper around OpenAI's chat completion API, mimicing the official ChatGPT webapp's functionality as closely as possible.\n   *\n   * @param apiKey - OpenAI API key (required).\n   * @param apiOrg - Optional OpenAI API organization (optional).\n   * @param apiBaseUrl - Optional override for the OpenAI API base URL.\n   * @param debug - Optional enables logging debugging info to stdout.\n   * @param completionParams - Param overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   * @param maxModelTokens - Optional override for the maximum number of tokens allowed by the model's context. Defaults to 4096.\n   * @param maxResponseTokens - Optional override for the minimum number of tokens allowed for the model's response. Defaults to 1000.\n   * @param messageStore - Optional [Keyv](https://github.com/jaredwray/keyv) store to persist chat messages to. If not provided, messages will be lost when the process exits.\n   * @param getMessageById - Optional function to retrieve a message by its ID. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param upsertMessage - Optional function to insert or update a message. If not provided, the default implementation will be used (using an in-memory `messageStore`).\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  constructor(opts: types.ChatGPTAPIOptions) {\n    const {\n      apiKey,\n      apiOrg,\n      apiBaseUrl = 'https://api.openai.com/v1',\n      debug = false,\n      messageStore,\n      completionParams,\n      systemMessage,\n      maxModelTokens = 4000,\n      maxResponseTokens = 1000,\n      getMessageById,\n      upsertMessage,\n      fetch = globalFetch\n    } = opts\n\n    this._apiKey = apiKey\n    this._apiOrg = apiOrg\n    this._apiBaseUrl = apiBaseUrl\n    this._debug = !!debug\n    this._fetch = fetch\n\n    this._completionParams = {\n      model: CHATGPT_MODEL,\n      temperature: 0.8,\n      top_p: 1.0,\n      presence_penalty: 1.0,\n      ...completionParams\n    }\n\n    this._systemMessage = systemMessage\n\n    if (this._systemMessage === undefined) {\n      const currentDate = new Date().toISOString().split('T')[0]\n      this._systemMessage = `You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible.\\nKnowledge cutoff: 2021-09-01\\nCurrent date: ${currentDate}`\n    }\n\n    this._maxModelTokens = maxModelTokens\n    this._maxResponseTokens = maxResponseTokens\n\n    this._getMessageById = getMessageById ?? this._defaultGetMessageById\n    this._upsertMessage = upsertMessage ?? this._defaultUpsertMessage\n\n    if (messageStore) {\n      this._messageStore = messageStore\n    } else {\n      this._messageStore = new Keyv<types.ChatMessage, any>({\n        store: new QuickLRU<string, types.ChatMessage>({ maxSize: 10000 })\n      })\n    }\n\n    if (!this._apiKey) {\n      throw new Error('OpenAI missing required apiKey')\n    }\n\n    if (!this._fetch) {\n      throw new Error('Invalid environment; fetch is not defined')\n    }\n\n    if (typeof this._fetch !== 'function') {\n      throw new Error('Invalid \"fetch\" is not a function')\n    }\n  }\n\n  /**\n   * Sends a message to the OpenAI chat completions endpoint, waits for the response\n   * to resolve, and returns the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI chat completions API. You can override the `systemMessage` in `opts` to customize the assistant's instructions.\n   *\n   * @param message - The prompt message to send\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.conversationId - Optional ID of the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.systemMessage - Optional override for the chat \"system message\" which acts as instructions to the model (defaults to the ChatGPT system message)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   * @param completionParams - Optional overrides to send to the [OpenAI chat completion API](https://platform.openai.com/docs/api-reference/chat/create). Options like `temperature` and `presence_penalty` can be tweaked to change the personality of the assistant.\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(\n    text: string,\n    opts: types.SendMessageOptions = {}\n  ): Promise<types.ChatMessage> {\n    const {\n      parentMessageId,\n      messageId = uuidv4(),\n      timeoutMs,\n      onProgress,\n      stream = onProgress ? true : false,\n      completionParams,\n      conversationId\n    } = opts\n\n    let { abortSignal } = opts\n\n    let abortController: AbortController = null\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController()\n      abortSignal = abortController.signal\n    }\n\n    const message: types.ChatMessage = {\n      role: 'user',\n      id: messageId,\n      conversationId,\n      parentMessageId,\n      text\n    }\n\n    const latestQuestion = message\n\n    const { messages, maxTokens, numTokens } = await this._buildMessages(\n      text,\n      opts\n    )\n\n    const result: types.ChatMessage = {\n      role: 'assistant',\n      id: uuidv4(),\n      conversationId,\n      parentMessageId: messageId,\n      text: ''\n    }\n\n    const responseP = new Promise<types.ChatMessage>(\n      async (resolve, reject) => {\n        const url = `${this._apiBaseUrl}/chat/completions`\n        const headers = {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${this._apiKey}`\n        }\n        const body = {\n          max_tokens: maxTokens,\n          ...this._completionParams,\n          ...completionParams,\n          messages,\n          stream\n        }\n\n        // Support multiple organizations\n        // See https://platform.openai.com/docs/api-reference/authentication\n        if (this._apiOrg) {\n          headers['OpenAI-Organization'] = this._apiOrg\n        }\n\n        if (this._debug) {\n          console.log(`sendMessage (${numTokens} tokens)`, body)\n        }\n\n        if (stream) {\n          fetchSSE(\n            url,\n            {\n              method: 'POST',\n              headers,\n              body: JSON.stringify(body),\n              signal: abortSignal,\n              onMessage: (data: string) => {\n                if (data === '[DONE]') {\n                  result.text = result.text.trim()\n                  return resolve(result)\n                }\n\n                try {\n                  const response: types.openai.CreateChatCompletionDeltaResponse =\n                    JSON.parse(data)\n\n                  if (response.id) {\n                    result.id = response.id\n                  }\n\n                  if (response.choices?.length) {\n                    const delta = response.choices[0].delta\n                    result.delta = delta.content\n                    if (delta?.content) result.text += delta.content\n\n                    if (delta.role) {\n                      result.role = delta.role\n                    }\n\n                    result.detail = response\n                    onProgress?.(result)\n                  }\n                } catch (err) {\n                  console.warn('OpenAI stream SEE event unexpected error', err)\n                  return reject(err)\n                }\n              }\n            },\n            this._fetch\n          ).catch(reject)\n        } else {\n          try {\n            const res = await this._fetch(url, {\n              method: 'POST',\n              headers,\n              body: JSON.stringify(body),\n              signal: abortSignal\n            })\n\n            if (!res.ok) {\n              const reason = await res.text()\n              const msg = `OpenAI error ${\n                res.status || res.statusText\n              }: ${reason}`\n              const error = new types.ChatGPTError(msg, { cause: res })\n              error.statusCode = res.status\n              error.statusText = res.statusText\n              return reject(error)\n            }\n\n            const response: types.openai.CreateChatCompletionResponse =\n              await res.json()\n            if (this._debug) {\n              console.log(response)\n            }\n\n            if (response?.id) {\n              result.id = response.id\n            }\n\n            if (response?.choices?.length) {\n              const message = response.choices[0].message\n              result.text = message.content\n              if (message.role) {\n                result.role = message.role\n              }\n            } else {\n              const res = response as any\n              return reject(\n                new Error(\n                  `OpenAI error: ${\n                    res?.detail?.message || res?.detail || 'unknown'\n                  }`\n                )\n              )\n            }\n\n            result.detail = response\n\n            return resolve(result)\n          } catch (err) {\n            return reject(err)\n          }\n        }\n      }\n    ).then(async (message) => {\n      if (message.detail && !message.detail.usage) {\n        try {\n          const promptTokens = numTokens\n          const completionTokens = await this._getTokenCount(message.text)\n          message.detail.usage = {\n            prompt_tokens: promptTokens,\n            completion_tokens: completionTokens,\n            total_tokens: promptTokens + completionTokens,\n            estimated: true\n          }\n        } catch (err) {\n          // TODO: this should really never happen, but if it does,\n          // we should handle notify the user gracefully\n        }\n      }\n\n      return Promise.all([\n        this._upsertMessage(latestQuestion),\n        this._upsertMessage(message)\n      ]).then(() => message)\n    })\n\n    if (timeoutMs) {\n      if (abortController) {\n        // This will be called when a timeout occurs in order for us to forcibly\n        // ensure that the underlying HTTP request is aborted.\n        ;(responseP as any).cancel = () => {\n          abortController.abort()\n        }\n      }\n\n      return pTimeout(responseP, {\n        milliseconds: timeoutMs,\n        message: 'OpenAI timed out waiting for response'\n      })\n    } else {\n      return responseP\n    }\n  }\n\n  get apiKey(): string {\n    return this._apiKey\n  }\n\n  set apiKey(apiKey: string) {\n    this._apiKey = apiKey\n  }\n\n  get apiOrg(): string {\n    return this._apiOrg\n  }\n\n  set apiOrg(apiOrg: string) {\n    this._apiOrg = apiOrg\n  }\n\n  protected async _buildMessages(text: string, opts: types.SendMessageOptions) {\n    const { systemMessage = this._systemMessage } = opts\n    let { parentMessageId } = opts\n\n    const userLabel = USER_LABEL_DEFAULT\n    const assistantLabel = ASSISTANT_LABEL_DEFAULT\n\n    const maxNumTokens = this._maxModelTokens - this._maxResponseTokens\n    let messages: types.openai.ChatCompletionRequestMessage[] = []\n\n    if (systemMessage) {\n      messages.push({\n        role: 'system',\n        content: systemMessage\n      })\n    }\n\n    const systemMessageOffset = messages.length\n    let nextMessages = text\n      ? messages.concat([\n          {\n            role: 'user',\n            content: text,\n            name: opts.name\n          }\n        ])\n      : messages\n    let numTokens = 0\n\n    do {\n      const prompt = nextMessages\n        .reduce((prompt, message) => {\n          switch (message.role) {\n            case 'system':\n              return prompt.concat([`Instructions:\\n${message.content}`])\n            case 'user':\n              return prompt.concat([`${userLabel}:\\n${message.content}`])\n            default:\n              return prompt.concat([`${assistantLabel}:\\n${message.content}`])\n          }\n        }, [] as string[])\n        .join('\\n\\n')\n\n      const nextNumTokensEstimate = await this._getTokenCount(prompt)\n      const isValidPrompt = nextNumTokensEstimate <= maxNumTokens\n\n      if (prompt && !isValidPrompt) {\n        break\n      }\n\n      messages = nextMessages\n      numTokens = nextNumTokensEstimate\n\n      if (!isValidPrompt) {\n        break\n      }\n\n      if (!parentMessageId) {\n        break\n      }\n\n      const parentMessage = await this._getMessageById(parentMessageId)\n      if (!parentMessage) {\n        break\n      }\n\n      const parentMessageRole = parentMessage.role || 'user'\n\n      nextMessages = nextMessages.slice(0, systemMessageOffset).concat([\n        {\n          role: parentMessageRole,\n          content: parentMessage.text,\n          name: parentMessage.name\n        },\n        ...nextMessages.slice(systemMessageOffset)\n      ])\n\n      parentMessageId = parentMessage.parentMessageId\n    } while (true)\n\n    // Use up to 4096 tokens (prompt + response), but try to leave 1000 tokens\n    // for the response.\n    const maxTokens = Math.max(\n      1,\n      Math.min(this._maxModelTokens - numTokens, this._maxResponseTokens)\n    )\n\n    return { messages, maxTokens, numTokens }\n  }\n\n  protected async _getTokenCount(text: string) {\n    // TODO: use a better fix in the tokenizer\n    text = text.replace(/<\\|endoftext\\|>/g, '')\n\n    return tokenizer.encode(text).length\n  }\n\n  protected async _defaultGetMessageById(\n    id: string\n  ): Promise<types.ChatMessage> {\n    const res = await this._messageStore.get(id)\n    return res\n  }\n\n  protected async _defaultUpsertMessage(\n    message: types.ChatMessage\n  ): Promise<void> {\n    await this._messageStore.set(message.id, message)\n  }\n}\n","import { get_encoding } from '@dqbd/tiktoken'\n\n// TODO: make this configurable\nconst tokenizer = get_encoding('cl100k_base')\n\nexport function encode(input: string): Uint32Array {\n  return tokenizer.encode(input)\n}\n","import Keyv from 'keyv'\n\nexport type Role = 'user' | 'assistant' | 'system'\n\nexport type FetchFn = typeof fetch\n\nexport type ChatGPTAPIOptions = {\n  apiKey: string\n\n  /** @defaultValue `'https://api.openai.com'` **/\n  apiBaseUrl?: string\n\n  apiOrg?: string\n\n  /** @defaultValue `false` **/\n  debug?: boolean\n\n  completionParams?: Partial<\n    Omit<openai.CreateChatCompletionRequest, 'messages' | 'n' | 'stream'>\n  >\n\n  systemMessage?: string\n\n  /** @defaultValue `4096` **/\n  maxModelTokens?: number\n\n  /** @defaultValue `1000` **/\n  maxResponseTokens?: number\n\n  messageStore?: Keyv\n  getMessageById?: GetMessageByIdFunction\n  upsertMessage?: UpsertMessageFunction\n\n  fetch?: FetchFn\n}\n\nexport type SendMessageOptions = {\n  /** The name of a user in a multi-user chat. */\n  name?: string\n  parentMessageId?: string\n  conversationId?: string\n  messageId?: string\n  stream?: boolean\n  systemMessage?: string\n  timeoutMs?: number\n  onProgress?: (partialResponse: ChatMessage) => void\n  abortSignal?: AbortSignal\n  completionParams?: Partial<\n    Omit<openai.CreateChatCompletionRequest, 'messages' | 'n' | 'stream'>\n  >\n}\n\nexport type MessageActionType = 'next' | 'variant'\n\nexport type SendMessageBrowserOptions = {\n  conversationId?: string\n  parentMessageId?: string\n  messageId?: string\n  action?: MessageActionType\n  timeoutMs?: number\n  onProgress?: (partialResponse: ChatMessage) => void\n  abortSignal?: AbortSignal\n}\n\nexport interface ChatMessage {\n  id: string\n  text: string\n  role: Role\n  name?: string\n  delta?: string\n  detail?:\n    | openai.CreateChatCompletionResponse\n    | CreateChatCompletionStreamResponse\n\n  // relevant for both ChatGPTAPI and ChatGPTUnofficialProxyAPI\n  parentMessageId?: string\n\n  // only relevant for ChatGPTUnofficialProxyAPI (optional for ChatGPTAPI)\n  conversationId?: string\n}\n\nexport class ChatGPTError extends Error {\n  statusCode?: number\n  statusText?: string\n  isFinal?: boolean\n  accountId?: string\n}\n\n/** Returns a chat message from a store by it's ID (or null if not found). */\nexport type GetMessageByIdFunction = (id: string) => Promise<ChatMessage>\n\n/** Upserts a chat message to a store. */\nexport type UpsertMessageFunction = (message: ChatMessage) => Promise<void>\n\nexport interface CreateChatCompletionStreamResponse\n  extends openai.CreateChatCompletionDeltaResponse {\n  usage: CreateCompletionStreamResponseUsage\n}\n\nexport interface CreateCompletionStreamResponseUsage\n  extends openai.CreateCompletionResponseUsage {\n  estimated: true\n}\n\n/**\n * https://chat.openapi.com/backend-api/conversation\n */\nexport type ConversationJSONBody = {\n  /**\n   * The action to take\n   */\n  action: string\n\n  /**\n   * The ID of the conversation\n   */\n  conversation_id?: string\n\n  /**\n   * Prompts to provide\n   */\n  messages: Prompt[]\n\n  /**\n   * The model to use\n   */\n  model: string\n\n  /**\n   * The parent message ID\n   */\n  parent_message_id: string\n}\n\nexport type Prompt = {\n  /**\n   * The content of the prompt\n   */\n  content: PromptContent\n\n  /**\n   * The ID of the prompt\n   */\n  id: string\n\n  /**\n   * The role played in the prompt\n   */\n  role: Role\n}\n\nexport type ContentType = 'text'\n\nexport type PromptContent = {\n  /**\n   * The content type of the prompt\n   */\n  content_type: ContentType\n\n  /**\n   * The parts to the prompt\n   */\n  parts: string[]\n}\n\nexport type ConversationResponseEvent = {\n  message?: Message\n  conversation_id?: string\n  error?: string | null\n}\n\nexport type Message = {\n  id: string\n  content: MessageContent\n  role: Role\n  user: string | null\n  create_time: string | null\n  update_time: string | null\n  end_turn: null\n  weight: number\n  recipient: string\n  metadata: MessageMetadata\n}\n\nexport type MessageContent = {\n  content_type: string\n  parts: string[]\n}\n\nexport type MessageMetadata = any\n\nexport namespace openai {\n  export interface CreateChatCompletionDeltaResponse {\n    id: string\n    object: 'chat.completion.chunk'\n    created: number\n    model: string\n    choices: [\n      {\n        delta: {\n          role: Role\n          content?: string\n        }\n        index: number\n        finish_reason: string | null\n      }\n    ]\n  }\n\n  /**\n   *\n   * @export\n   * @interface ChatCompletionRequestMessage\n   */\n  export interface ChatCompletionRequestMessage {\n    /**\n     * The role of the author of this message.\n     * @type {string}\n     * @memberof ChatCompletionRequestMessage\n     */\n    role: ChatCompletionRequestMessageRoleEnum\n    /**\n     * The contents of the message\n     * @type {string}\n     * @memberof ChatCompletionRequestMessage\n     */\n    content: string\n    /**\n     * The name of the user in a multi-user chat\n     * @type {string}\n     * @memberof ChatCompletionRequestMessage\n     */\n    name?: string\n  }\n  export declare const ChatCompletionRequestMessageRoleEnum: {\n    readonly System: 'system'\n    readonly User: 'user'\n    readonly Assistant: 'assistant'\n  }\n  export declare type ChatCompletionRequestMessageRoleEnum =\n    (typeof ChatCompletionRequestMessageRoleEnum)[keyof typeof ChatCompletionRequestMessageRoleEnum]\n  /**\n   *\n   * @export\n   * @interface ChatCompletionResponseMessage\n   */\n  export interface ChatCompletionResponseMessage {\n    /**\n     * The role of the author of this message.\n     * @type {string}\n     * @memberof ChatCompletionResponseMessage\n     */\n    role: ChatCompletionResponseMessageRoleEnum\n    /**\n     * The contents of the message\n     * @type {string}\n     * @memberof ChatCompletionResponseMessage\n     */\n    content: string\n  }\n  export declare const ChatCompletionResponseMessageRoleEnum: {\n    readonly System: 'system'\n    readonly User: 'user'\n    readonly Assistant: 'assistant'\n  }\n  export declare type ChatCompletionResponseMessageRoleEnum =\n    (typeof ChatCompletionResponseMessageRoleEnum)[keyof typeof ChatCompletionResponseMessageRoleEnum]\n  /**\n   *\n   * @export\n   * @interface CreateChatCompletionRequest\n   */\n  export interface CreateChatCompletionRequest {\n    /**\n     * ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.\n     * @type {string}\n     * @memberof CreateChatCompletionRequest\n     */\n    model: string\n    /**\n     * The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).\n     * @type {Array<ChatCompletionRequestMessage>}\n     * @memberof CreateChatCompletionRequest\n     */\n    messages: Array<ChatCompletionRequestMessage>\n    /**\n     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.  We generally recommend altering this or `top_p` but not both.\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    temperature?: number | null\n    /**\n     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    top_p?: number | null\n    /**\n     * How many chat completion choices to generate for each input message.\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    n?: number | null\n    /**\n     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\n     * @type {boolean}\n     * @memberof CreateChatCompletionRequest\n     */\n    stream?: boolean | null\n    /**\n     *\n     * @type {CreateChatCompletionRequestStop}\n     * @memberof CreateChatCompletionRequest\n     */\n    stop?: CreateChatCompletionRequestStop\n    /**\n     * The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    max_tokens?: number\n    /**\n     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model\\'s likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    presence_penalty?: number | null\n    /**\n     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\\'s likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n     * @type {number}\n     * @memberof CreateChatCompletionRequest\n     */\n    frequency_penalty?: number | null\n    /**\n     * Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n     * @type {object}\n     * @memberof CreateChatCompletionRequest\n     */\n    logit_bias?: object | null\n    /**\n     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n     * @type {string}\n     * @memberof CreateChatCompletionRequest\n     */\n    user?: string\n  }\n  /**\n   * @type CreateChatCompletionRequestStop\n   * Up to 4 sequences where the API will stop generating further tokens.\n   * @export\n   */\n  export declare type CreateChatCompletionRequestStop = Array<string> | string\n  /**\n   *\n   * @export\n   * @interface CreateChatCompletionResponse\n   */\n  export interface CreateChatCompletionResponse {\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponse\n     */\n    id: string\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponse\n     */\n    object: string\n    /**\n     *\n     * @type {number}\n     * @memberof CreateChatCompletionResponse\n     */\n    created: number\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponse\n     */\n    model: string\n    /**\n     *\n     * @type {Array<CreateChatCompletionResponseChoicesInner>}\n     * @memberof CreateChatCompletionResponse\n     */\n    choices: Array<CreateChatCompletionResponseChoicesInner>\n    /**\n     *\n     * @type {CreateCompletionResponseUsage}\n     * @memberof CreateChatCompletionResponse\n     */\n    usage?: CreateCompletionResponseUsage\n  }\n  /**\n   *\n   * @export\n   * @interface CreateChatCompletionResponseChoicesInner\n   */\n  export interface CreateChatCompletionResponseChoicesInner {\n    /**\n     *\n     * @type {number}\n     * @memberof CreateChatCompletionResponseChoicesInner\n     */\n    index?: number\n    /**\n     *\n     * @type {ChatCompletionResponseMessage}\n     * @memberof CreateChatCompletionResponseChoicesInner\n     */\n    message?: ChatCompletionResponseMessage\n    /**\n     *\n     * @type {string}\n     * @memberof CreateChatCompletionResponseChoicesInner\n     */\n    finish_reason?: string\n  }\n  /**\n   *\n   * @export\n   * @interface CreateCompletionResponseUsage\n   */\n  export interface CreateCompletionResponseUsage {\n    /**\n     *\n     * @type {number}\n     * @memberof CreateCompletionResponseUsage\n     */\n    prompt_tokens: number\n    /**\n     *\n     * @type {number}\n     * @memberof CreateCompletionResponseUsage\n     */\n    completion_tokens: number\n    /**\n     *\n     * @type {number}\n     * @memberof CreateCompletionResponseUsage\n     */\n    total_tokens: number\n  }\n}\n","/// <reference lib=\"dom\" />\n\nconst fetch = globalThis.fetch\n\nexport { fetch }\n","import { createParser } from 'eventsource-parser'\n\nimport * as types from './types'\nimport { fetch as globalFetch } from './fetch'\nimport { streamAsyncIterable } from './stream-async-iterable'\n\nexport async function fetchSSE(\n  url: string,\n  options: Parameters<typeof fetch>[1] & {\n    onMessage: (data: string) => void\n    onError?: (error: any) => void\n  },\n  fetch: types.FetchFn = globalFetch\n) {\n  const { onMessage, onError, ...fetchOptions } = options\n  const res = await fetch(url, fetchOptions)\n  if (!res.ok) {\n    let reason: string\n\n    try {\n      reason = await res.text()\n    } catch (err) {\n      reason = res.statusText\n    }\n\n    const msg = `ChatGPT error ${res.status}: ${reason}`\n    const error = new types.ChatGPTError(msg, { cause: res })\n    error.statusCode = res.status\n    error.statusText = res.statusText\n    throw error\n  }\n\n  const parser = createParser((event) => {\n    if (event.type === 'event') {\n      onMessage(event.data)\n    }\n  })\n\n  // handle special response errors\n  const feed = (chunk: string) => {\n    let response = null\n\n    try {\n      response = JSON.parse(chunk)\n    } catch {\n      // ignore\n    }\n\n    if (response?.detail?.type === 'invalid_request_error') {\n      const msg = `ChatGPT error ${response.detail.message}: ${response.detail.code} (${response.detail.type})`\n      const error = new types.ChatGPTError(msg, { cause: response })\n      error.statusCode = response.detail.code\n      error.statusText = response.detail.message\n\n      if (onError) {\n        onError(error)\n      } else {\n        console.error(error)\n      }\n\n      // don't feed to the event parser\n      return\n    }\n\n    parser.feed(chunk)\n  }\n\n  if (!res.body.getReader) {\n    // Vercel polyfills `fetch` with `node-fetch`, which doesn't conform to\n    // web standards, so this is a workaround...\n    const body: NodeJS.ReadableStream = res.body as any\n\n    if (!body.on || !body.read) {\n      throw new types.ChatGPTError('unsupported \"fetch\" implementation')\n    }\n\n    body.on('readable', () => {\n      let chunk: string | Buffer\n      while (null !== (chunk = body.read())) {\n        feed(chunk.toString())\n      }\n    })\n  } else {\n    for await (const chunk of streamAsyncIterable(res.body)) {\n      const str = new TextDecoder().decode(chunk)\n      feed(str)\n    }\n  }\n}\n","export async function* streamAsyncIterable<T>(stream: ReadableStream<T>) {\n  const reader = stream.getReader()\n  try {\n    while (true) {\n      const { done, value } = await reader.read()\n      if (done) {\n        return\n      }\n      yield value\n    }\n  } finally {\n    reader.releaseLock()\n  }\n}\n","import pTimeout from 'p-timeout'\nimport { v4 as uuidv4 } from 'uuid'\n\nimport * as types from './types'\nimport { fetch as globalFetch } from './fetch'\nimport { fetchSSE } from './fetch-sse'\nimport { isValidUUIDv4 } from './utils'\n\nexport class ChatGPTUnofficialProxyAPI {\n  protected _accessToken: string\n  protected _apiReverseProxyUrl: string\n  protected _debug: boolean\n  protected _model: string\n  protected _headers: Record<string, string>\n  protected _fetch: types.FetchFn\n\n  /**\n   * @param fetch - Optional override for the `fetch` implementation to use. Defaults to the global `fetch` function.\n   */\n  constructor(opts: {\n    accessToken: string\n\n    /** @defaultValue `https://bypass.duti.tech/api/conversation` **/\n    apiReverseProxyUrl?: string\n\n    /** @defaultValue `text-davinci-002-render-sha` **/\n    model?: string\n\n    /** @defaultValue `false` **/\n    debug?: boolean\n\n    /** @defaultValue `undefined` **/\n    headers?: Record<string, string>\n\n    fetch?: types.FetchFn\n  }) {\n    const {\n      accessToken,\n      apiReverseProxyUrl = 'https://bypass.duti.tech/api/conversation',\n      model = 'text-davinci-002-render-sha',\n      debug = false,\n      headers,\n      fetch = globalFetch\n    } = opts\n\n    this._accessToken = accessToken\n    this._apiReverseProxyUrl = apiReverseProxyUrl\n    this._debug = !!debug\n    this._model = model\n    this._fetch = fetch\n    this._headers = headers\n\n    if (!this._accessToken) {\n      throw new Error('ChatGPT invalid accessToken')\n    }\n\n    if (!this._fetch) {\n      throw new Error('Invalid environment; fetch is not defined')\n    }\n\n    if (typeof this._fetch !== 'function') {\n      throw new Error('Invalid \"fetch\" is not a function')\n    }\n  }\n\n  get accessToken(): string {\n    return this._accessToken\n  }\n\n  set accessToken(value: string) {\n    this._accessToken = value\n  }\n\n  /**\n   * Sends a message to ChatGPT, waits for the response to resolve, and returns\n   * the response.\n   *\n   * If you want your response to have historical context, you must provide a valid `parentMessageId`.\n   *\n   * If you want to receive a stream of partial responses, use `opts.onProgress`.\n   * If you want to receive the full response, including message and conversation IDs,\n   * you can use `opts.onConversationResponse` or use the `ChatGPTAPI.getConversation`\n   * helper.\n   *\n   * Set `debug: true` in the `ChatGPTAPI` constructor to log more info on the full prompt sent to the OpenAI completions API. You can override the `promptPrefix` and `promptSuffix` in `opts` to customize the prompt.\n   *\n   * @param message - The prompt message to send\n   * @param opts.conversationId - Optional ID of a conversation to continue (defaults to a random UUID)\n   * @param opts.parentMessageId - Optional ID of the previous message in the conversation (defaults to `undefined`)\n   * @param opts.messageId - Optional ID of the message to send (defaults to a random UUID)\n   * @param opts.timeoutMs - Optional timeout in milliseconds (defaults to no timeout)\n   * @param opts.onProgress - Optional callback which will be invoked every time the partial response is updated\n   * @param opts.abortSignal - Optional callback used to abort the underlying `fetch` call using an [AbortController](https://developer.mozilla.org/en-US/docs/Web/API/AbortController)\n   *\n   * @returns The response from ChatGPT\n   */\n  async sendMessage(\n    text: string,\n    opts: types.SendMessageBrowserOptions = {}\n  ): Promise<types.ChatMessage> {\n    if (!!opts.conversationId !== !!opts.parentMessageId) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: conversationId and parentMessageId must both be set or both be undefined'\n      )\n    }\n\n    if (opts.conversationId && !isValidUUIDv4(opts.conversationId)) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: conversationId is not a valid v4 UUID'\n      )\n    }\n\n    if (opts.parentMessageId && !isValidUUIDv4(opts.parentMessageId)) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: parentMessageId is not a valid v4 UUID'\n      )\n    }\n\n    if (opts.messageId && !isValidUUIDv4(opts.messageId)) {\n      throw new Error(\n        'ChatGPTUnofficialProxyAPI.sendMessage: messageId is not a valid v4 UUID'\n      )\n    }\n\n    const {\n      conversationId,\n      parentMessageId = uuidv4(),\n      messageId = uuidv4(),\n      action = 'next',\n      timeoutMs,\n      onProgress\n    } = opts\n\n    let { abortSignal } = opts\n\n    let abortController: AbortController = null\n    if (timeoutMs && !abortSignal) {\n      abortController = new AbortController()\n      abortSignal = abortController.signal\n    }\n\n    const body: types.ConversationJSONBody = {\n      action,\n      messages: [\n        {\n          id: messageId,\n          role: 'user',\n          content: {\n            content_type: 'text',\n            parts: [text]\n          }\n        }\n      ],\n      model: this._model,\n      parent_message_id: parentMessageId\n    }\n\n    if (conversationId) {\n      body.conversation_id = conversationId\n    }\n\n    const result: types.ChatMessage = {\n      role: 'assistant',\n      id: uuidv4(),\n      parentMessageId: messageId,\n      conversationId,\n      text: ''\n    }\n\n    const responseP = new Promise<types.ChatMessage>((resolve, reject) => {\n      const url = this._apiReverseProxyUrl\n      const headers = {\n        ...this._headers,\n        Authorization: `Bearer ${this._accessToken}`,\n        Accept: 'text/event-stream',\n        'Content-Type': 'application/json'\n      }\n\n      if (this._debug) {\n        console.log('POST', url, { body, headers })\n      }\n\n      fetchSSE(\n        url,\n        {\n          method: 'POST',\n          headers,\n          body: JSON.stringify(body),\n          signal: abortSignal,\n          onMessage: (data: string) => {\n            if (data === '[DONE]') {\n              return resolve(result)\n            }\n\n            try {\n              const convoResponseEvent: types.ConversationResponseEvent =\n                JSON.parse(data)\n              if (convoResponseEvent.conversation_id) {\n                result.conversationId = convoResponseEvent.conversation_id\n              }\n\n              if (convoResponseEvent.message?.id) {\n                result.id = convoResponseEvent.message.id\n              }\n\n              const message = convoResponseEvent.message\n              // console.log('event', JSON.stringify(convoResponseEvent, null, 2))\n\n              if (message) {\n                let text = message?.content?.parts?.[0]\n\n                if (text) {\n                  result.text = text\n\n                  if (onProgress) {\n                    onProgress(result)\n                  }\n                }\n              }\n            } catch (err) {\n              if (this._debug) {\n                console.warn('chatgpt unexpected JSON error', err)\n              }\n              // reject(err)\n            }\n          },\n          onError: (err) => {\n            reject(err)\n          }\n        },\n        this._fetch\n      ).catch((err) => {\n        const errMessageL = err.toString().toLowerCase()\n\n        if (\n          result.text &&\n          (errMessageL === 'error: typeerror: terminated' ||\n            errMessageL === 'typeerror: terminated')\n        ) {\n          // OpenAI sometimes forcefully terminates the socket from their end before\n          // the HTTP request has resolved cleanly. In my testing, these cases tend to\n          // happen when OpenAI has already send the last `response`, so we can ignore\n          // the `fetch` error in this case.\n          return resolve(result)\n        } else {\n          return reject(err)\n        }\n      })\n    })\n\n    if (timeoutMs) {\n      if (abortController) {\n        // This will be called when a timeout occurs in order for us to forcibly\n        // ensure that the underlying HTTP request is aborted.\n        ;(responseP as any).cancel = () => {\n          abortController.abort()\n        }\n      }\n\n      return pTimeout(responseP, {\n        milliseconds: timeoutMs,\n        message: 'ChatGPT timed out waiting for response'\n      })\n    } else {\n      return responseP\n    }\n  }\n}\n","const uuidv4Re =\n  /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i\n\nexport function isValidUUIDv4(str: string): boolean {\n  return str && uuidv4Re.test(str)\n}\n"],"mappings":";;;;;;;;;;;;;;;AAAA,OAAOA,IAAA,MAAU;AACjB,OAAOC,QAAA,MAAc;AACrB,OAAOC,QAAA,MAAc;AACrB,SAASC,EAAA,IAAMC,MAAA,QAAc;;;ACH7B,SAASC,YAAA,QAAoB;AAG7B,IAAMC,SAAA,GAAYD,YAAA,CAAa,aAAa;AAErC,SAASE,OAAOC,KAAA,EAA4B;EACjD,OAAOF,SAAA,CAAUC,MAAA,CAAOC,KAAK;AAC/B;;;AC0EO,IAAMC,YAAA,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,YAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,YAAA;EAAA,SAAAA,aAAA;IAAAK,eAAA,OAAAL,YAAA;IAAA,OAAAG,MAAA,CAAAG,KAAA,OAAAC,SAAA;EAAA;EAAA,OAAAC,YAAA,CAAAR,YAAA;AAAA,gBAAAS,gBAAA,CAAqBC,KAAA,EAKlC;AAyGO,IAAUC,MAAA;AAAA,CAAV,UAAUC,OAAA,EAAV,IAAUD,MAAA,KAAAA,MAAA;;;AC7LjB,IAAME,KAAA,GAAQC,UAAA,CAAWD,KAAA;;;ACFzB,SAASE,YAAA,QAAoB;;;SCANC,oBAAAC,EAAA;EAAA,OAAAC,oBAAA,CAAAZ,KAAA,OAAAC,SAAA;AAAA,E;;sFAAvB,SAAAY,QAA8CC,MAAA;IAAA,IAAAC,MAAA,EAAAC,qBAAA,EAAAC,IAAA,EAAAC,KAAA;IAAA,OAAAC,mBAAA,GAAAC,IAAA,UAAAC,SAAAC,QAAA;MAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;QAAA;UACtCT,MAAA,GAASD,MAAA,CAAOW,SAAA,CAAU;UAAAH,QAAA,CAAAC,IAAA;QAAA;UAAA,KAEvB;YAAAD,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA,OAAAE,oBAAA,CACyBX,MAAA,CAAOY,IAAA,CAAK;QAAA;UAAAX,qBAAA,GAAAM,QAAA,CAAAM,IAAA;UAAlCX,IAAA,GAAAD,qBAAA,CAAAC,IAAA;UAAMC,KAAA,GAAAF,qBAAA,CAAAE,KAAA;UAAA,KACVD,IAAA;YAAAK,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAA,OAAAF,QAAA,CAAAO,MAAA;QAAA;UAAAP,QAAA,CAAAE,IAAA;UAGJ,OAAMN,KAAA;QAAA;UAAAI,QAAA,CAAAE,IAAA;UAAA;QAAA;UAAAF,QAAA,CAAAC,IAAA;UAGRR,MAAA,CAAOe,WAAA,CAAY;UAAA,OAAAR,QAAA,CAAAS,MAAA;QAAA;QAAA;UAAA,OAAAT,QAAA,CAAAU,IAAA;MAAA;IAAA,GAAAnB,OAAA;EAAA,CAEvB;EAAA,OAAAD,oBAAA,CAAAZ,KAAA,OAAAC,SAAA;AAAA;AAAA,SDPsBgC,SAAAC,GAAA,EAAAC,GAAA;EAAA,OAAAC,SAAA,CAAApC,KAAA,OAAAC,SAAA;AAAA,E;;yEAAtB,SAAAoC,UACEC,GAAA,EACAC,OAAA;IAAA,IAAAC,MAAA;MAAAC,SAAA;MAAAC,OAAA;MAAAC,YAAA;MAAAC,GAAA;MAAAC,MAAA;MAAAC,GAAA;MAAAC,KAAA;MAAAC,MAAA;MAAAC,IAAA;MAAAC,IAAA;MAAAC,yBAAA;MAAAC,iBAAA;MAAAC,cAAA;MAAAC,SAAA;MAAAC,KAAA;MAAAC,KAAA;MAAAC,GAAA;MAAAC,OAAA,GAAAzD,SAAA;IAAA,OAAAkB,mBAAA,GAAAC,IAAA,UAAAuC,WAAAC,UAAA;MAAA,kBAAAA,UAAA,CAAArC,IAAA,GAAAqC,UAAA,CAAApC,IAAA;QAAA;UAIAgB,MAAA,GAAAkB,OAAA,CAAAG,MAAA,QAAAH,OAAA,QAAAI,SAAA,GAAAJ,OAAA,MAAuBnD,KAAA;UAEfkC,SAAA,GAAwCF,OAAA,CAAxCE,SAAA,EAAWC,OAAA,GAA6BH,OAAA,CAA7BG,OAAA,EAAYC,YAAA,GAAAoB,wBAAA,CAAiBxB,OAAA,EAAAyB,SAAA;UAAAJ,UAAA,CAAApC,IAAA;UAAA,OAC9BgB,MAAA,CAAMF,GAAA,EAAKK,YAAY;QAAA;UAAnCC,GAAA,GAAAgB,UAAA,CAAAhC,IAAA;UAAA,IACDgB,GAAA,CAAIqB,EAAA;YAAAL,UAAA,CAAApC,IAAA;YAAA;UAAA;UAAAoC,UAAA,CAAArC,IAAA;UAAAqC,UAAA,CAAApC,IAAA;UAAA,OAIUoB,GAAA,CAAIsB,IAAA,CAAK;QAAA;UAAxBrB,MAAA,GAAAe,UAAA,CAAAhC,IAAA;UAAAgC,UAAA,CAAApC,IAAA;UAAA;QAAA;UAAAoC,UAAA,CAAArC,IAAA;UAAAqC,UAAA,CAAAO,EAAA,GAAAP,UAAA;UAEAf,MAAA,GAASD,GAAA,CAAIwB,UAAA;QAAA;UAGTtB,GAAA,oBAAAuB,MAAA,CAAuBzB,GAAA,CAAI0B,MAAA,QAAAD,MAAA,CAAWxB,MAAA;UACtCE,KAAA,GAAQ,IAAUrD,YAAA,CAAaoD,GAAA,EAAK;YAAEyB,KAAA,EAAO3B;UAAI,CAAC;UACxDG,KAAA,CAAMyB,UAAA,GAAa5B,GAAA,CAAI0B,MAAA;UACvBvB,KAAA,CAAMqB,UAAA,GAAaxB,GAAA,CAAIwB,UAAA;UAAA,MACjBrB,KAAA;QAAA;UAGFC,MAAA,GAASvC,YAAA,CAAa,UAACgE,KAAA,EAAU;YACrC,IAAIA,KAAA,CAAMC,IAAA,KAAS,SAAS;cAC1BjC,SAAA,CAAUgC,KAAA,CAAME,IAAI;YACtB;UACF,CAAC;UAGK1B,IAAA,GAAO,SAAPA,KAAQO,KAAA,EAAkB;YAvClC,IAAAoB,EAAA;YAwCI,IAAIC,QAAA,GAAW;YAEf,IAAI;cACFA,QAAA,GAAWC,IAAA,CAAKC,KAAA,CAAMvB,KAAK;YAC7B,SAAAwB,OAAA,EAAE,CAEF;YAEA,MAAIJ,EAAA,GAAAC,QAAA,oBAAAA,QAAA,CAAUI,MAAA,KAAV,gBAAAL,EAAA,CAAkBF,IAAA,MAAS,yBAAyB;cACtD,IAAM5B,IAAA,oBAAAuB,MAAA,CAAuBQ,QAAA,CAASI,MAAA,CAAOC,OAAA,QAAAb,MAAA,CAAYQ,QAAA,CAASI,MAAA,CAAOE,IAAA,QAAAd,MAAA,CAASQ,QAAA,CAASI,MAAA,CAAOP,IAAA;cAClG,IAAM3B,MAAA,GAAQ,IAAUrD,YAAA,CAAaoD,IAAA,EAAK;gBAAEyB,KAAA,EAAOM;cAAS,CAAC;cAC7D9B,MAAA,CAAMyB,UAAA,GAAaK,QAAA,CAASI,MAAA,CAAOE,IAAA;cACnCpC,MAAA,CAAMqB,UAAA,GAAaS,QAAA,CAASI,MAAA,CAAOC,OAAA;cAEnC,IAAIxC,OAAA,EAAS;gBACXA,OAAA,CAAQK,MAAK;cACf,OAAO;gBACLqC,OAAA,CAAQrC,KAAA,CAAMA,MAAK;cACrB;cAGA;YACF;YAEAC,MAAA,CAAOC,IAAA,CAAKO,KAAK;UACnB;UAAA,IAEKZ,GAAA,CAAIM,IAAA,CAAKzB,SAAA;YAAAmC,UAAA,CAAApC,IAAA;YAAA;UAAA;UAGN0B,IAAA,GAA8BN,GAAA,CAAIM,IAAA;UAAA,MAEpC,CAACA,IAAA,CAAKmC,EAAA,IAAM,CAACnC,IAAA,CAAKvB,IAAA;YAAAiC,UAAA,CAAApC,IAAA;YAAA;UAAA;UAAA,MACd,IAAU9B,YAAA,CAAa,oCAAoC;QAAA;UAGnEwD,IAAA,CAAKmC,EAAA,CAAG,YAAY,YAAM;YACxB,IAAI7B,KAAA;YACJ,OAAO,UAAUA,KAAA,GAAQN,IAAA,CAAKvB,IAAA,CAAK,IAAI;cACrCsB,IAAA,CAAKO,KAAA,CAAM8B,QAAA,CAAS,CAAC;YACvB;UACF,CAAC;UAAA1B,UAAA,CAAApC,IAAA;UAAA;QAAA;UAAA2B,yBAAA;UAAAC,iBAAA;UAAAQ,UAAA,CAAArC,IAAA;UAAA+B,SAAA,GAAAiC,cAAA,CAEyB7E,mBAAA,CAAoBkC,GAAA,CAAIM,IAAI;QAAA;UAAAU,UAAA,CAAApC,IAAA;UAAA,OAAA8B,SAAA,CAAA9B,IAAA;QAAA;UAAA,MAAA2B,yBAAA,KAAAI,KAAA,GAAAK,UAAA,CAAAhC,IAAA,EAAAX,IAAA;YAAA2C,UAAA,CAAApC,IAAA;YAAA;UAAA;UAArCgC,KAAA,GAAAD,KAAA,CAAArC,KAAA;UACTuC,GAAA,GAAM,IAAI+B,WAAA,CAAY,EAAEC,MAAA,CAAOjC,KAAK;UAC1CP,IAAA,CAAKQ,GAAG;QAAA;UAAAN,yBAAA;UAAAS,UAAA,CAAApC,IAAA;UAAA;QAAA;UAAAoC,UAAA,CAAApC,IAAA;UAAA;QAAA;UAAAoC,UAAA,CAAArC,IAAA;UAAAqC,UAAA,CAAA8B,EAAA,GAAA9B,UAAA;UAAAR,iBAAA;UAAAC,cAAA,GAAAO,UAAA,CAAA8B,EAAA;QAAA;UAAA9B,UAAA,CAAArC,IAAA;UAAAqC,UAAA,CAAArC,IAAA;UAAA,MAAA4B,yBAAA,IAAAG,SAAA,CAAAqC,MAAA;YAAA/B,UAAA,CAAApC,IAAA;YAAA;UAAA;UAAAoC,UAAA,CAAApC,IAAA;UAAA,OAAA8B,SAAA,CAAAqC,MAAA;QAAA;UAAA/B,UAAA,CAAArC,IAAA;UAAA,KAAA6B,iBAAA;YAAAQ,UAAA,CAAApC,IAAA;YAAA;UAAA;UAAA,MAAA6B,cAAA;QAAA;UAAA,OAAAO,UAAA,CAAA7B,MAAA;QAAA;UAAA,OAAA6B,UAAA,CAAA7B,MAAA;QAAA;QAAA;UAAA,OAAA6B,UAAA,CAAA5B,IAAA;MAAA;IAAA,GAAAK,SAAA;EAAA,CAGd;EAAA,OAAAD,SAAA,CAAApC,KAAA,OAAAC,SAAA;AAAA;AJ9EA,IAAM2F,aAAA,GAAgB;AAEtB,IAAMC,kBAAA,GAAqB;AAC3B,IAAMC,uBAAA,GAA0B;AAEzB,IAAMC,UAAA;EAAW;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;EAmCtB,SAAAA,WAAYC,IAAA,EAA+B;IAAAjG,eAAA,OAAAgG,UAAA;IACzC,IACEE,MAAA,GAYED,IAAA,CAZFC,MAAA;MACAC,MAAA,GAWEF,IAAA,CAXFE,MAAA;MAAAC,gBAAA,GAWEH,IAAA,CAVFI,UAAA;MAAAA,UAAA,GAAAD,gBAAA,cAAa,8BAAAA,gBAAA;MAAAE,WAAA,GAUXL,IAAA,CATFM,KAAA;MAAAA,KAAA,GAAAD,WAAA,cAAQ,QAAAA,WAAA;MACRE,YAAA,GAQEP,IAAA,CARFO,YAAA;MACAC,gBAAA,GAOER,IAAA,CAPFQ,gBAAA;MACAC,aAAA,GAMET,IAAA,CANFS,aAAA;MAAAC,oBAAA,GAMEV,IAAA,CALFW,cAAA;MAAAA,cAAA,GAAAD,oBAAA,cAAiB,MAAAA,oBAAA;MAAAE,qBAAA,GAKfZ,IAAA,CAJFa,iBAAA;MAAAA,iBAAA,GAAAD,qBAAA,cAAoB,MAAAA,qBAAA;MACpBE,cAAA,GAGEd,IAAA,CAHFc,cAAA;MACAC,aAAA,GAEEf,IAAA,CAFFe,aAAA;MAAAC,WAAA,GAEEhB,IAAA,CADFzF,KAAA;MAAAiC,MAAA,GAAAwE,WAAA,cAAQzG,KAAA,GAAAyG,WAAA;IAGV,KAAKC,OAAA,GAAUhB,MAAA;IACf,KAAKiB,OAAA,GAAUhB,MAAA;IACf,KAAKiB,WAAA,GAAcf,UAAA;IACnB,KAAKgB,MAAA,GAAS,CAAC,CAACd,KAAA;IAChB,KAAKe,MAAA,GAAS7E,MAAA;IAEd,KAAK8E,iBAAA,GAAAC,aAAA;MACHC,KAAA,EAAO5B,aAAA;MACP6B,WAAA,EAAa;MACbC,KAAA,EAAO;MACPC,gBAAA,EAAkB;IAAA,GACfnB,gBAAA,CACL;IAEA,KAAKoB,cAAA,GAAiBnB,aAAA;IAEtB,IAAI,KAAKmB,cAAA,KAAmB,QAAW;MACrC,IAAMC,WAAA,GAAc,mBAAIC,IAAA,CAAK,EAAEC,WAAA,CAAY,EAAEC,KAAA,CAAM,GAAG,EAAE,CAAC;MACzD,KAAKJ,cAAA,+IAAAvD,MAAA,CAA6JwD,WAAA;IACpK;IAEA,KAAKI,eAAA,GAAkBtB,cAAA;IACvB,KAAKuB,kBAAA,GAAqBrB,iBAAA;IAE1B,KAAKsB,eAAA,GAAkBrB,cAAA,aAAAA,cAAA,cAAAA,cAAA,GAAkB,KAAKsB,sBAAA;IAC9C,KAAKC,cAAA,GAAiBtB,aAAA,aAAAA,aAAA,cAAAA,aAAA,GAAiB,KAAKuB,qBAAA;IAE5C,IAAI/B,YAAA,EAAc;MAChB,KAAKgC,aAAA,GAAgBhC,YAAA;IACvB,OAAO;MACL,KAAKgC,aAAA,GAAgB,IAAItJ,IAAA,CAA6B;QACpDuJ,KAAA,EAAO,IAAIrJ,QAAA,CAAoC;UAAEsJ,OAAA,EAAS;QAAM,CAAC;MACnE,CAAC;IACH;IAEA,IAAI,CAAC,KAAKxB,OAAA,EAAS;MACjB,MAAM,IAAI7G,KAAA,CAAM,gCAAgC;IAClD;IAEA,IAAI,CAAC,KAAKiH,MAAA,EAAQ;MAChB,MAAM,IAAIjH,KAAA,CAAM,2CAA2C;IAC7D;IAEA,IAAI,OAAO,KAAKiH,MAAA,KAAW,YAAY;MACrC,MAAM,IAAIjH,KAAA,CAAM,mCAAmC;IACrD;EACF;EAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;EAAAF,YAAA,CAAA6F,UAAA;IAAA2C,GAAA;IAAAxH,KAAA;MAAA,IAAAyH,YAAA,GAAAC,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAwBA,SAAAC,SACE5E,IAAA;QAAA,IAAA6E,KAAA;QAAA,IAAA/C,IAAA;UAAAgD,eAAA;UAAAC,eAAA;UAAAC,SAAA;UAAAC,SAAA;UAAAC,UAAA;UAAAC,YAAA;UAAAvI,MAAA;UAAA0F,gBAAA;UAAA8C,cAAA;UAAAC,WAAA;UAAAC,eAAA;UAAAtE,OAAA;UAAAuE,cAAA;UAAAC,qBAAA;UAAAC,QAAA;UAAAC,SAAA;UAAAC,SAAA;UAAAC,MAAA;UAAAC,SAAA;UAAAC,MAAA,GAAA/J,SAAA;QAAA,OAAAkB,mBAAA,GAAAC,IAAA,UAAA6I,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA3I,IAAA,GAAA2I,SAAA,CAAA1I,IAAA;YAAA;cACAwE,IAAA,GAAAgE,MAAA,CAAAnG,MAAA,QAAAmG,MAAA,QAAAlG,SAAA,GAAAkG,MAAA,MAAiC,CAAC;cAGhChB,eAAA,GAOEhD,IAAA,CAPFgD,eAAA,EAAAC,eAAA,GAOEjD,IAAA,CANFkD,SAAA,EAAAA,SAAA,GAAAD,eAAA,cAAY5J,MAAA,CAAO,IAAA4J,eAAA,EACnBE,SAAA,GAKEnD,IAAA,CALFmD,SAAA,EACAC,UAAA,GAIEpD,IAAA,CAJFoD,UAAA,EAAAC,YAAA,GAIErD,IAAA,CAHFlF,MAAA,EAAAA,MAAA,GAAAuI,YAAA,cAASD,UAAA,GAAa,OAAO,QAAAC,YAAA,EAC7B7C,gBAAA,GAEER,IAAA,CAFFQ,gBAAA,EACA8C,cAAA,GACEtD,IAAA,CADFsD,cAAA;cAGIC,WAAA,GAAgBvD,IAAA,CAAhBuD,WAAA;cAEFC,eAAA,GAAmC;cACvC,IAAIL,SAAA,IAAa,CAACI,WAAA,EAAa;gBAC7BC,eAAA,GAAkB,IAAIW,eAAA,CAAgB;gBACtCZ,WAAA,GAAcC,eAAA,CAAgBY,MAAA;cAChC;cAEMlF,OAAA,GAA6B;gBACjCmF,IAAA,EAAM;gBACNC,EAAA,EAAIpB,SAAA;gBACJI,cAAA,EAAAA,cAAA;gBACAN,eAAA,EAAAA,eAAA;gBACA9E,IAAA,EAAAA;cACF;cAEMuF,cAAA,GAAiBvE,OAAA;cAAAgF,SAAA,CAAA1I,IAAA;cAAA,OAE0B,KAAK+I,cAAA,CACpDrG,IAAA,EACA8B,IACF;YAAA;cAAA0D,qBAAA,GAAAQ,SAAA,CAAAtI,IAAA;cAHQ+H,QAAA,GAAAD,qBAAA,CAAAC,QAAA;cAAUC,SAAA,GAAAF,qBAAA,CAAAE,SAAA;cAAWC,SAAA,GAAAH,qBAAA,CAAAG,SAAA;cAKvBC,MAAA,GAA4B;gBAChCO,IAAA,EAAM;gBACNC,EAAA,EAAIjL,MAAA,CAAO;gBACXiK,cAAA,EAAAA,cAAA;gBACAN,eAAA,EAAiBE,SAAA;gBACjBhF,IAAA,EAAM;cACR;cAEM6F,SAAA,GAAY,IAAIS,OAAA;gBAAA,IAAAC,IAAA,GAAA7B,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CACpB,SAAA6B,SAAOC,OAAA,EAASC,MAAA;kBAAA,IAAAhG,EAAA,EAAAiG,EAAA,EAAAvI,GAAA,EAAAwI,OAAA,EAAA5H,IAAA,EAAAN,GAAA,EAAAC,MAAA,EAAAC,GAAA,EAAAC,KAAA,EAAA8B,QAAA,EAAAkG,QAAA,EAAAC,IAAA;kBAAA,OAAA7J,mBAAA,GAAAC,IAAA,UAAA6J,UAAAC,SAAA;oBAAA,kBAAAA,SAAA,CAAA3J,IAAA,GAAA2J,SAAA,CAAA1J,IAAA;sBAAA;wBACRc,GAAA,MAAA+B,MAAA,CAAS0E,KAAA,CAAK5B,WAAA;wBACd2D,OAAA,GAAU;0BACd,gBAAgB;0BAChBK,aAAA,YAAA9G,MAAA,CAAyB0E,KAAA,CAAK9B,OAAA;wBAChC;wBACM/D,IAAA,GAAAqE,aAAA,CAAAA,aAAA,CAAAA,aAAA;0BACJ6D,UAAA,EAAYxB;wBAAA,GACTb,KAAA,CAAKzB,iBAAA,GACLd,gBAAA;0BACHmD,QAAA,EAAAA,QAAA;0BACA7I,MAAA,EAAAA;wBAAA;wBAKF,IAAIiI,KAAA,CAAK7B,OAAA,EAAS;0BAChB4D,OAAA,CAAQ,qBAAqB,IAAI/B,KAAA,CAAK7B,OAAA;wBACxC;wBAEA,IAAI6B,KAAA,CAAK3B,MAAA,EAAQ;0BACfhC,OAAA,CAAQiG,GAAA,iBAAAhH,MAAA,CAAoBwF,SAAA,eAAqB3G,IAAI;wBACvD;wBAAA,KAEIpC,MAAA;0BAAAoK,SAAA,CAAA1J,IAAA;0BAAA;wBAAA;wBACFS,QAAA,CACEK,GAAA,EACA;0BACEgJ,MAAA,EAAQ;0BACRR,OAAA,EAAAA,OAAA;0BACA5H,IAAA,EAAM4B,IAAA,CAAKyG,SAAA,CAAUrI,IAAI;0BACzBkH,MAAA,EAAQb,WAAA;0BACR9G,SAAA,EAAW,SAAAA,UAACkC,IAAA,EAAiB;4BAtN3C,IAAA6G,GAAA;4BAuNgB,IAAI7G,IAAA,KAAS,UAAU;8BACrBmF,MAAA,CAAO5F,IAAA,GAAO4F,MAAA,CAAO5F,IAAA,CAAKuH,IAAA,CAAK;8BAC/B,OAAOd,OAAA,CAAQb,MAAM;4BACvB;4BAEA,IAAI;8BACF,IAAMjF,QAAA,GACJC,IAAA,CAAKC,KAAA,CAAMJ,IAAI;8BAEjB,IAAIE,QAAA,CAASyF,EAAA,EAAI;gCACfR,MAAA,CAAOQ,EAAA,GAAKzF,QAAA,CAASyF,EAAA;8BACvB;8BAEA,KAAIkB,GAAA,GAAA3G,QAAA,CAAS6G,OAAA,KAAT,gBAAAF,GAAA,CAAkB3H,MAAA,EAAQ;gCAC5B,IAAM8H,KAAA,GAAQ9G,QAAA,CAAS6G,OAAA,CAAQ,CAAC,EAAEC,KAAA;gCAClC7B,MAAA,CAAO6B,KAAA,GAAQA,KAAA,CAAMC,OAAA;gCACrB,IAAID,KAAA,oBAAAA,KAAA,CAAOC,OAAA,EAAS9B,MAAA,CAAO5F,IAAA,IAAQyH,KAAA,CAAMC,OAAA;gCAEzC,IAAID,KAAA,CAAMtB,IAAA,EAAM;kCACdP,MAAA,CAAOO,IAAA,GAAOsB,KAAA,CAAMtB,IAAA;gCACtB;gCAEAP,MAAA,CAAO7E,MAAA,GAASJ,QAAA;gCAChBuE,UAAA,oBAAAA,UAAA,CAAaU,MAAA;8BACf;4BACF,SAAS+B,GAAA,EAAP;8BACAzG,OAAA,CAAQ0G,IAAA,CAAK,4CAA4CD,GAAG;8BAC5D,OAAOjB,MAAA,CAAOiB,GAAG;4BACnB;0BACF;wBACF,GACA9C,KAAA,CAAK1B,MACP,EAAE0E,KAAA,CAAMnB,MAAM;wBAAAM,SAAA,CAAA1J,IAAA;wBAAA;sBAAA;wBAAA0J,SAAA,CAAA3J,IAAA;wBAAA2J,SAAA,CAAA1J,IAAA;wBAAA,OAGMuH,KAAA,CAAK1B,MAAA,CAAO/E,GAAA,EAAK;0BACjCgJ,MAAA,EAAQ;0BACRR,OAAA,EAAAA,OAAA;0BACA5H,IAAA,EAAM4B,IAAA,CAAKyG,SAAA,CAAUrI,IAAI;0BACzBkH,MAAA,EAAQb;wBACV,CAAC;sBAAA;wBALK3G,GAAA,GAAAsI,SAAA,CAAAtJ,IAAA;wBAAA,IAODgB,GAAA,CAAIqB,EAAA;0BAAAiH,SAAA,CAAA1J,IAAA;0BAAA;wBAAA;wBAAA0J,SAAA,CAAA1J,IAAA;wBAAA,OACcoB,GAAA,CAAIsB,IAAA,CAAK;sBAAA;wBAAxBrB,MAAA,GAAAqI,SAAA,CAAAtJ,IAAA;wBACAkB,GAAA,mBAAAuB,MAAA,CACJzB,GAAA,CAAI0B,MAAA,IAAU1B,GAAA,CAAIwB,UAAA,QAAAC,MAAA,CACfxB,MAAA;wBACCE,KAAA,GAAQ,IAAUrD,YAAA,CAAaoD,GAAA,EAAK;0BAAEyB,KAAA,EAAO3B;wBAAI,CAAC;wBACxDG,KAAA,CAAMyB,UAAA,GAAa5B,GAAA,CAAI0B,MAAA;wBACvBvB,KAAA,CAAMqB,UAAA,GAAaxB,GAAA,CAAIwB,UAAA;wBAAA,OAAA8G,SAAA,CAAArJ,MAAA,WAChB+I,MAAA,CAAO7H,KAAK;sBAAA;wBAAAmI,SAAA,CAAA1J,IAAA;wBAAA,OAIboB,GAAA,CAAIoJ,IAAA,CAAK;sBAAA;wBADXnH,QAAA,GAAAqG,SAAA,CAAAtJ,IAAA;wBAEN,IAAImH,KAAA,CAAK3B,MAAA,EAAQ;0BACfhC,OAAA,CAAQiG,GAAA,CAAIxG,QAAQ;wBACtB;wBAEA,IAAIA,QAAA,oBAAAA,QAAA,CAAUyF,EAAA,EAAI;0BAChBR,MAAA,CAAOQ,EAAA,GAAKzF,QAAA,CAASyF,EAAA;wBACvB;wBAAA,MAEA,CAAI1F,EAAA,GAAAC,QAAA,oBAAAA,QAAA,CAAU6G,OAAA,KAAV,gBAAA9G,EAAA,CAAmBf,MAAA;0BAAAqH,SAAA,CAAA1J,IAAA;0BAAA;wBAAA;wBACfuJ,QAAA,GAAUlG,QAAA,CAAS6G,OAAA,CAAQ,CAAC,EAAExG,OAAA;wBACpC4E,MAAA,CAAO5F,IAAA,GAAO6G,QAAA,CAAQa,OAAA;wBACtB,IAAIb,QAAA,CAAQV,IAAA,EAAM;0BAChBP,MAAA,CAAOO,IAAA,GAAOU,QAAA,CAAQV,IAAA;wBACxB;wBAAAa,SAAA,CAAA1J,IAAA;wBAAA;sBAAA;wBAEMwJ,IAAA,GAAMnG,QAAA;wBAAA,OAAAqG,SAAA,CAAArJ,MAAA,WACL+I,MAAA,CACL,IAAIxK,KAAA,kBAAAiE,MAAA,CACF,EACEwG,EAAA,GAAAG,IAAA,oBAAAA,IAAA,CAAK/F,MAAA,KAAL,gBAAA4F,EAAA,CAAa3F,OAAA,MAAW8F,IAAA,oBAAAA,IAAA,CAAK/F,MAAA,KAAU,UAE3C,CACF;sBAAA;wBAGF6E,MAAA,CAAO7E,MAAA,GAASJ,QAAA;wBAAA,OAAAqG,SAAA,CAAArJ,MAAA,WAET8I,OAAA,CAAQb,MAAM;sBAAA;wBAAAoB,SAAA,CAAA3J,IAAA;wBAAA2J,SAAA,CAAA/G,EAAA,GAAA+G,SAAA;wBAAA,OAAAA,SAAA,CAAArJ,MAAA,WAEd+I,MAAA,CAAAM,SAAA,CAAA/G,EAAU;sBAAA;sBAAA;wBAAA,OAAA+G,SAAA,CAAAlJ,IAAA;oBAAA;kBAAA,GAAA0I,QAAA;gBAAA,CAGvB;gBAAA,iBAAAuB,GAAA,EAAAC,GAAA;kBAAA,OAAAzB,IAAA,CAAAzK,KAAA,OAAAC,SAAA;gBAAA;cAAA,GACF,EAAEkM,IAAA;gBAAA,IAAAC,KAAA,GAAAxD,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAAK,SAAAwD,SAAOtB,QAAA;kBAAA,IAAAuB,YAAA,EAAAC,gBAAA;kBAAA,OAAApL,mBAAA,GAAAC,IAAA,UAAAoL,UAAAC,SAAA;oBAAA,kBAAAA,SAAA,CAAAlL,IAAA,GAAAkL,SAAA,CAAAjL,IAAA;sBAAA;wBAAA,MACRuJ,QAAA,CAAQ9F,MAAA,IAAU,CAAC8F,QAAA,CAAQ9F,MAAA,CAAOyH,KAAA;0BAAAD,SAAA,CAAAjL,IAAA;0BAAA;wBAAA;wBAAAiL,SAAA,CAAAlL,IAAA;wBAE5B+K,YAAA,GAAezC,SAAA;wBAAA4C,SAAA,CAAAjL,IAAA;wBAAA,OACUuH,KAAA,CAAK4D,cAAA,CAAe5B,QAAA,CAAQ7G,IAAI;sBAAA;wBAAzDqI,gBAAA,GAAAE,SAAA,CAAA7K,IAAA;wBACNmJ,QAAA,CAAQ9F,MAAA,CAAOyH,KAAA,GAAQ;0BACrBE,aAAA,EAAeN,YAAA;0BACfO,iBAAA,EAAmBN,gBAAA;0BACnBO,YAAA,EAAcR,YAAA,GAAeC,gBAAA;0BAC7BQ,SAAA,EAAW;wBACb;wBAAAN,SAAA,CAAAjL,IAAA;wBAAA;sBAAA;wBAAAiL,SAAA,CAAAlL,IAAA;wBAAAkL,SAAA,CAAAtI,EAAA,GAAAsI,SAAA;sBAAA;wBAAA,OAAAA,SAAA,CAAA5K,MAAA,WAOG2I,OAAA,CAAQwC,GAAA,CAAI,CACjBjE,KAAA,CAAKV,cAAA,CAAeoB,cAAc,GAClCV,KAAA,CAAKV,cAAA,CAAe0C,QAAO,EAC5B,EAAEoB,IAAA,CAAK;0BAAA,OAAMpB,QAAO;wBAAA;sBAAA;sBAAA;wBAAA,OAAA0B,SAAA,CAAAzK,IAAA;oBAAA;kBAAA,GAAAqK,QAAA;gBAAA,CACtB;gBAAA,iBAAAY,GAAA;kBAAA,OAAAb,KAAA,CAAApM,KAAA,OAAAC,SAAA;gBAAA;cAAA;cAAA,KAEGkJ,SAAA;gBAAAe,SAAA,CAAA1I,IAAA;gBAAA;cAAA;cACF,IAAIgI,eAAA,EAAiB;gBAGnB;gBAAEO,SAAA,CAAkBmD,MAAA,GAAS,YAAM;kBACjC1D,eAAA,CAAgB2D,KAAA,CAAM;gBACxB;cACF;cAAA,OAAAjD,SAAA,CAAArI,MAAA,WAEO3C,QAAA,CAAS6K,SAAA,EAAW;gBACzBqD,YAAA,EAAcjE,SAAA;gBACdjE,OAAA,EAAS;cACX,CAAC;YAAA;cAAA,OAAAgF,SAAA,CAAArI,MAAA,WAEMkI,SAAA;YAAA;YAAA;cAAA,OAAAG,SAAA,CAAAlI,IAAA;UAAA;QAAA,GAAA8G,QAAA;MAAA,CAEX;MAAA,SAAAuE,YAAAC,GAAA;QAAA,OAAA3E,YAAA,CAAA3I,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAoN,WAAA;IAAA;EAAA;IAAA3E,GAAA;IAAA6E,GAAA,EAEA,SAAAA,IAAA,EAAqB;MACnB,OAAO,KAAKtG,OAAA;IACd;IAAAuG,GAAA,EAEA,SAAAA,IAAWvH,MAAA,EAAgB;MACzB,KAAKgB,OAAA,GAAUhB,MAAA;IACjB;EAAA;IAAAyC,GAAA;IAAA6E,GAAA,EAEA,SAAAA,IAAA,EAAqB;MACnB,OAAO,KAAKrG,OAAA;IACd;IAAAsG,GAAA,EAEA,SAAAA,IAAWtH,MAAA,EAAgB;MACzB,KAAKgB,OAAA,GAAUhB,MAAA;IACjB;EAAA;IAAAwC,GAAA;IAAAxH,KAAA;MAAA,IAAAuM,eAAA,GAAA7E,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAEA,SAAA6E,SAA+BxJ,IAAA,EAAc8B,IAAA;QAAA,IAAA2H,mBAAA,EAAAlH,aAAA,EAAAuC,eAAA,EAAA4E,SAAA,EAAAC,cAAA,EAAAC,YAAA,EAAAnE,QAAA,EAAAoE,mBAAA,EAAAC,YAAA,EAAAnE,SAAA,EAAAoE,MAAA,EAAAC,qBAAA,EAAAC,aAAA,EAAAC,aAAA,EAAAC,iBAAA,EAAAzE,SAAA;QAAA,OAAAzI,mBAAA,GAAAC,IAAA,UAAAkN,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAhN,IAAA,GAAAgN,SAAA,CAAA/M,IAAA;YAAA;cAAAmM,mBAAA,GACK3H,IAAA,CAAxCS,aAAA,EAAAA,aAAA,GAAAkH,mBAAA,cAAgB,KAAK/F,cAAA,GAAA+F,mBAAA;cACvB3E,eAAA,GAAoBhD,IAAA,CAApBgD,eAAA;cAEA4E,SAAA,GAAY/H,kBAAA;cACZgI,cAAA,GAAiB/H,uBAAA;cAEjBgI,YAAA,GAAe,KAAK7F,eAAA,GAAkB,KAAKC,kBAAA;cAC7CyB,QAAA,GAAwD,EAAC;cAE7D,IAAIlD,aAAA,EAAe;gBACjBkD,QAAA,CAAS6E,IAAA,CAAK;kBACZnE,IAAA,EAAM;kBACNuB,OAAA,EAASnF;gBACX,CAAC;cACH;cAEMsH,mBAAA,GAAsBpE,QAAA,CAAS9F,MAAA;cACjCmK,YAAA,GAAe9J,IAAA,GACfyF,QAAA,CAAStF,MAAA,CAAO,CACd;gBACEgG,IAAA,EAAM;gBACNuB,OAAA,EAAS1H,IAAA;gBACTuK,IAAA,EAAMzI,IAAA,CAAKyI;cACb,EACD,IACD9E,QAAA;cACAE,SAAA,GAAY;YAAA;cAGRoE,MAAA,GAASD,YAAA,CACZU,MAAA,CAAO,UAACC,OAAA,EAAQzJ,OAAA,EAAY;gBAC3B,QAAQA,OAAA,CAAQmF,IAAA;kBACd,KAAK;oBACH,OAAOsE,OAAA,CAAOtK,MAAA,CAAO,mBAAAA,MAAA,CAAmBa,OAAA,CAAQ0G,OAAA,EAAU;kBAC5D,KAAK;oBACH,OAAO+C,OAAA,CAAOtK,MAAA,CAAO,IAAAA,MAAA,CAAIuJ,SAAA,SAAAvJ,MAAA,CAAea,OAAA,CAAQ0G,OAAA,EAAU;kBAC5D;oBACE,OAAO+C,OAAA,CAAOtK,MAAA,CAAO,IAAAA,MAAA,CAAIwJ,cAAA,SAAAxJ,MAAA,CAAoBa,OAAA,CAAQ0G,OAAA,EAAU;gBACnE;cACF,GAAG,EAAc,EAChBgD,IAAA,CAAK,MAAM;cAAAL,SAAA,CAAA/M,IAAA;cAAA,OAEsB,KAAKmL,cAAA,CAAesB,MAAM;YAAA;cAAxDC,qBAAA,GAAAK,SAAA,CAAA3M,IAAA;cACAuM,aAAA,GAAgBD,qBAAA,IAAyBJ,YAAA;cAAA,MAE3CG,MAAA,IAAU,CAACE,aAAA;gBAAAI,SAAA,CAAA/M,IAAA;gBAAA;cAAA;cAAA,OAAA+M,SAAA,CAAA1M,MAAA;YAAA;cAIf8H,QAAA,GAAWqE,YAAA;cACXnE,SAAA,GAAYqE,qBAAA;cAAA,IAEPC,aAAA;gBAAAI,SAAA,CAAA/M,IAAA;gBAAA;cAAA;cAAA,OAAA+M,SAAA,CAAA1M,MAAA;YAAA;cAAA,IAIAmH,eAAA;gBAAAuF,SAAA,CAAA/M,IAAA;gBAAA;cAAA;cAAA,OAAA+M,SAAA,CAAA1M,MAAA;YAAA;cAAA0M,SAAA,CAAA/M,IAAA;cAAA,OAIuB,KAAK2G,eAAA,CAAgBa,eAAe;YAAA;cAA1DoF,aAAA,GAAAG,SAAA,CAAA3M,IAAA;cAAA,IACDwM,aAAA;gBAAAG,SAAA,CAAA/M,IAAA;gBAAA;cAAA;cAAA,OAAA+M,SAAA,CAAA1M,MAAA;YAAA;cAICwM,iBAAA,GAAoBD,aAAA,CAAc/D,IAAA,IAAQ;cAEhD2D,YAAA,GAAeA,YAAA,CAAaa,KAAA,CAAM,GAAGd,mBAAmB,EAAE1J,MAAA,EACxD;gBACEgG,IAAA,EAAMgE,iBAAA;gBACNzC,OAAA,EAASwC,aAAA,CAAclK,IAAA;gBACvBuK,IAAA,EAAML,aAAA,CAAcK;cACtB,GAAApK,MAAA,CAAAyK,kBAAA,CACGd,YAAA,CAAaa,KAAA,CAAMd,mBAAmB,GAC1C;cAED/E,eAAA,GAAkBoF,aAAA,CAAcpF,eAAA;YAAA;cAAA,IACzB;gBAAAuF,SAAA,CAAA/M,IAAA;gBAAA;cAAA;YAAA;cAIHoI,SAAA,GAAYmF,IAAA,CAAKC,GAAA,CACrB,GACAD,IAAA,CAAKE,GAAA,CAAI,KAAKhH,eAAA,GAAkB4B,SAAA,EAAW,KAAK3B,kBAAkB,CACpE;cAAA,OAAAqG,SAAA,CAAA1M,MAAA,WAEO;gBAAE8H,QAAA,EAAAA,QAAA;gBAAUC,SAAA,EAAAA,SAAA;gBAAWC,SAAA,EAAAA;cAAU;YAAA;YAAA;cAAA,OAAA0E,SAAA,CAAAvM,IAAA;UAAA;QAAA,GAAA0L,QAAA;MAAA,CAC1C;MAAA,SAAAnD,eAAA2E,GAAA,EAAAC,GAAA;QAAA,OAAA1B,eAAA,CAAAzN,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAsK,cAAA;IAAA;EAAA;IAAA7B,GAAA;IAAAxH,KAAA;MAAA,IAAAkO,eAAA,GAAAxG,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAEA,SAAAwG,SAA+BnL,IAAA;QAAA,OAAA/C,mBAAA,GAAAC,IAAA,UAAAkO,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAhO,IAAA,GAAAgO,SAAA,CAAA/N,IAAA;YAAA;cAE7B0C,IAAA,GAAOA,IAAA,CAAKsL,OAAA,CAAQ,oBAAoB,EAAE;cAAA,OAAAD,SAAA,CAAA1N,MAAA,WAEzBrC,MAAA,CAAO0E,IAAI,EAAEL,MAAA;YAAA;YAAA;cAAA,OAAA0L,SAAA,CAAAvN,IAAA;UAAA;QAAA,GAAAqN,QAAA;MAAA,CAChC;MAAA,SAAA1C,eAAA8C,IAAA;QAAA,OAAAL,eAAA,CAAApP,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAA0M,cAAA;IAAA;EAAA;IAAAjE,GAAA;IAAAxH,KAAA;MAAA,IAAAwO,uBAAA,GAAA9G,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAEA,SAAA8G,SACErF,EAAA;QAAA,IAAA1H,GAAA;QAAA,OAAAzB,mBAAA,GAAAC,IAAA,UAAAwO,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAtO,IAAA,GAAAsO,SAAA,CAAArO,IAAA;YAAA;cAAAqO,SAAA,CAAArO,IAAA;cAAA,OAEkB,KAAK+G,aAAA,CAAcgF,GAAA,CAAIjD,EAAE;YAAA;cAArC1H,GAAA,GAAAiN,SAAA,CAAAjO,IAAA;cAAA,OAAAiO,SAAA,CAAAhO,MAAA,WACCe,GAAA;YAAA;YAAA;cAAA,OAAAiN,SAAA,CAAA7N,IAAA;UAAA;QAAA,GAAA2N,QAAA;MAAA,CACT;MAAA,SAAAvH,uBAAA0H,IAAA;QAAA,OAAAJ,uBAAA,CAAA1P,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAmI,sBAAA;IAAA;EAAA;IAAAM,GAAA;IAAAxH,KAAA;MAAA,IAAA6O,sBAAA,GAAAnH,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAEA,SAAAmH,SACE9K,OAAA;QAAA,OAAA/D,mBAAA,GAAAC,IAAA,UAAA6O,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA3O,IAAA,GAAA2O,SAAA,CAAA1O,IAAA;YAAA;cAAA0O,SAAA,CAAA1O,IAAA;cAAA,OAEM,KAAK+G,aAAA,CAAciF,GAAA,CAAItI,OAAA,CAAQoF,EAAA,EAAIpF,OAAO;YAAA;YAAA;cAAA,OAAAgL,SAAA,CAAAlO,IAAA;UAAA;QAAA,GAAAgO,QAAA;MAAA,CAClD;MAAA,SAAA1H,sBAAA6H,IAAA;QAAA,OAAAJ,sBAAA,CAAA/P,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAqI,qBAAA;IAAA;EAAA;EAAA,OAAAvC,UAAA;AAAA,GACF;;;AMrdA,OAAOqK,SAAA,MAAc;AACrB,SAAShR,EAAA,IAAMiR,OAAA,QAAc;;;ACD7B,IAAMC,QAAA,GACJ;AAEK,SAASC,cAAc9M,GAAA,EAAsB;EAClD,OAAOA,GAAA,IAAO6M,QAAA,CAASE,IAAA,CAAK/M,GAAG;AACjC;;;ADGO,IAAMgN,yBAAA;EAA0B;AAAA;AAAA;EAWrC,SAAAA,0BAAYzK,IAAA,EAgBT;IAAAjG,eAAA,OAAA0Q,yBAAA;IACD,IACEC,WAAA,GAME1K,IAAA,CANF0K,WAAA;MAAAC,qBAAA,GAME3K,IAAA,CALF4K,kBAAA;MAAAA,kBAAA,GAAAD,qBAAA,cAAqB,8CAAAA,qBAAA;MAAAE,WAAA,GAKnB7K,IAAA,CAJFwB,KAAA;MAAAA,KAAA,GAAAqJ,WAAA,cAAQ,gCAAAA,WAAA;MAAAC,YAAA,GAIN9K,IAAA,CAHFM,KAAA;MAAAA,KAAA,GAAAwK,YAAA,cAAQ,QAAAA,YAAA;MACRhG,OAAA,GAEE9E,IAAA,CAFF8E,OAAA;MAAAiG,YAAA,GAEE/K,IAAA,CADFzF,KAAA;MAAAiC,MAAA,GAAAuO,YAAA,cAAQxQ,KAAA,GAAAwQ,YAAA;IAGV,KAAKC,YAAA,GAAeN,WAAA;IACpB,KAAKO,mBAAA,GAAsBL,kBAAA;IAC3B,KAAKxJ,MAAA,GAAS,CAAC,CAACd,KAAA;IAChB,KAAK4K,MAAA,GAAS1J,KAAA;IACd,KAAKH,MAAA,GAAS7E,MAAA;IACd,KAAK2O,QAAA,GAAWrG,OAAA;IAEhB,IAAI,CAAC,KAAKkG,YAAA,EAAc;MACtB,MAAM,IAAI5Q,KAAA,CAAM,6BAA6B;IAC/C;IAEA,IAAI,CAAC,KAAKiH,MAAA,EAAQ;MAChB,MAAM,IAAIjH,KAAA,CAAM,2CAA2C;IAC7D;IAEA,IAAI,OAAO,KAAKiH,MAAA,KAAW,YAAY;MACrC,MAAM,IAAIjH,KAAA,CAAM,mCAAmC;IACrD;EACF;EAAAF,YAAA,CAAAuQ,yBAAA;IAAA/H,GAAA;IAAA6E,GAAA,EAEA,SAAAA,IAAA,EAA0B;MACxB,OAAO,KAAKyD,YAAA;IACd;IAAAxD,GAAA,EAEA,SAAAA,IAAgBtM,KAAA,EAAe;MAC7B,KAAK8P,YAAA,GAAe9P,KAAA;IACtB;IAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;EAAA;IAAAwH,GAAA;IAAAxH,KAAA;MAAA,IAAAkQ,aAAA,GAAAxI,iBAAA,eAAAzH,mBAAA,GAAA0H,IAAA,CAyBA,SAAAwI,SACEnN,IAAA;QAAA,IAAAoN,MAAA;QAAA,IAAAtL,IAAA;UAAAsD,cAAA;UAAAiI,qBAAA;UAAAvI,eAAA;UAAAwI,gBAAA;UAAAtI,SAAA;UAAAuI,YAAA;UAAAC,MAAA;UAAAvI,SAAA;UAAAC,UAAA;UAAAG,WAAA;UAAAC,eAAA;UAAAtG,IAAA;UAAA4G,MAAA;UAAAC,SAAA;UAAA4H,MAAA,GAAA1R,SAAA;QAAA,OAAAkB,mBAAA,GAAAC,IAAA,UAAAwQ,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAtQ,IAAA,GAAAsQ,SAAA,CAAArQ,IAAA;YAAA;cACAwE,IAAA,GAAA2L,MAAA,CAAA9N,MAAA,QAAA8N,MAAA,QAAA7N,SAAA,GAAA6N,MAAA,MAAwC,CAAC;cAAA,MAErC,CAAC,CAAC3L,IAAA,CAAKsD,cAAA,KAAmB,CAAC,CAACtD,IAAA,CAAKgD,eAAA;gBAAA6I,SAAA,CAAArQ,IAAA;gBAAA;cAAA;cAAA,MAC7B,IAAIpB,KAAA,CACR,iHACF;YAAA;cAAA,MAGE4F,IAAA,CAAKsD,cAAA,IAAkB,CAACiH,aAAA,CAAcvK,IAAA,CAAKsD,cAAc;gBAAAuI,SAAA,CAAArQ,IAAA;gBAAA;cAAA;cAAA,MACrD,IAAIpB,KAAA,CACR,8EACF;YAAA;cAAA,MAGE4F,IAAA,CAAKgD,eAAA,IAAmB,CAACuH,aAAA,CAAcvK,IAAA,CAAKgD,eAAe;gBAAA6I,SAAA,CAAArQ,IAAA;gBAAA;cAAA;cAAA,MACvD,IAAIpB,KAAA,CACR,+EACF;YAAA;cAAA,MAGE4F,IAAA,CAAKkD,SAAA,IAAa,CAACqH,aAAA,CAAcvK,IAAA,CAAKkD,SAAS;gBAAA2I,SAAA,CAAArQ,IAAA;gBAAA;cAAA;cAAA,MAC3C,IAAIpB,KAAA,CACR,yEACF;YAAA;cAIAkJ,cAAA,GAMEtD,IAAA,CANFsD,cAAA,EAAAiI,qBAAA,GAMEvL,IAAA,CALFgD,eAAA,EAAAA,eAAA,GAAAuI,qBAAA,cAAkBlB,OAAA,CAAO,IAAAkB,qBAAA,EAAAC,gBAAA,GAKvBxL,IAAA,CAJFkD,SAAA,EAAAA,SAAA,GAAAsI,gBAAA,cAAYnB,OAAA,CAAO,IAAAmB,gBAAA,EAAAC,YAAA,GAIjBzL,IAAA,CAHF0L,MAAA,EAAAA,MAAA,GAAAD,YAAA,cAAS,SAAAA,YAAA,EACTtI,SAAA,GAEEnD,IAAA,CAFFmD,SAAA,EACAC,UAAA,GACEpD,IAAA,CADFoD,UAAA;cAGIG,WAAA,GAAgBvD,IAAA,CAAhBuD,WAAA;cAEFC,eAAA,GAAmC;cACvC,IAAIL,SAAA,IAAa,CAACI,WAAA,EAAa;gBAC7BC,eAAA,GAAkB,IAAIW,eAAA,CAAgB;gBACtCZ,WAAA,GAAcC,eAAA,CAAgBY,MAAA;cAChC;cAEMlH,IAAA,GAAmC;gBACvCwO,MAAA,EAAAA,MAAA;gBACA/H,QAAA,EAAU,CACR;kBACEW,EAAA,EAAIpB,SAAA;kBACJmB,IAAA,EAAM;kBACNuB,OAAA,EAAS;oBACPkG,YAAA,EAAc;oBACdC,KAAA,EAAO,CAAC7N,IAAI;kBACd;gBACF,EACF;gBACAsD,KAAA,EAAO,KAAK0J,MAAA;gBACZc,iBAAA,EAAmBhJ;cACrB;cAEA,IAAIM,cAAA,EAAgB;gBAClBpG,IAAA,CAAK+O,eAAA,GAAkB3I,cAAA;cACzB;cAEMQ,MAAA,GAA4B;gBAChCO,IAAA,EAAM;gBACNC,EAAA,EAAI+F,OAAA,CAAO;gBACXrH,eAAA,EAAiBE,SAAA;gBACjBI,cAAA,EAAAA,cAAA;gBACApF,IAAA,EAAM;cACR;cAEM6F,SAAA,GAAY,IAAIS,OAAA,CAA2B,UAACG,OAAA,EAASC,MAAA,EAAW;gBACpE,IAAMtI,GAAA,GAAMgP,MAAA,CAAKL,mBAAA;gBACjB,IAAMnG,OAAA,GAAAvD,aAAA,CAAAA,aAAA,KACD+J,MAAA,CAAKH,QAAA;kBACRhG,aAAA,YAAA9G,MAAA,CAAyBiN,MAAA,CAAKN,YAAA;kBAC9BkB,MAAA,EAAQ;kBACR,gBAAgB;gBAAA,EAClB;gBAEA,IAAIZ,MAAA,CAAKlK,MAAA,EAAQ;kBACfhC,OAAA,CAAQiG,GAAA,CAAI,QAAQ/I,GAAA,EAAK;oBAAEY,IAAA,EAAAA,IAAA;oBAAM4H,OAAA,EAAAA;kBAAQ,CAAC;gBAC5C;gBAEA7I,QAAA,CACEK,GAAA,EACA;kBACEgJ,MAAA,EAAQ;kBACRR,OAAA,EAAAA,OAAA;kBACA5H,IAAA,EAAM4B,IAAA,CAAKyG,SAAA,CAAUrI,IAAI;kBACzBkH,MAAA,EAAQb,WAAA;kBACR9G,SAAA,EAAW,SAAAA,UAACkC,IAAA,EAAiB;oBA7LvC,IAAAC,EAAA,EAAAiG,EAAA,EAAAsH,EAAA;oBA8LY,IAAIxN,IAAA,KAAS,UAAU;sBACrB,OAAOgG,OAAA,CAAQb,MAAM;oBACvB;oBAEA,IAAI;sBACF,IAAMsI,kBAAA,GACJtN,IAAA,CAAKC,KAAA,CAAMJ,IAAI;sBACjB,IAAIyN,kBAAA,CAAmBH,eAAA,EAAiB;wBACtCnI,MAAA,CAAOR,cAAA,GAAiB8I,kBAAA,CAAmBH,eAAA;sBAC7C;sBAEA,KAAIrN,EAAA,GAAAwN,kBAAA,CAAmBlN,OAAA,KAAnB,gBAAAN,EAAA,CAA4B0F,EAAA,EAAI;wBAClCR,MAAA,CAAOQ,EAAA,GAAK8H,kBAAA,CAAmBlN,OAAA,CAAQoF,EAAA;sBACzC;sBAEA,IAAMpF,OAAA,GAAUkN,kBAAA,CAAmBlN,OAAA;sBAGnC,IAAIA,OAAA,EAAS;wBACX,IAAImN,KAAA,IAAOF,EAAA,IAAAtH,EAAA,GAAA3F,OAAA,oBAAAA,OAAA,CAAS0G,OAAA,KAAT,gBAAAf,EAAA,CAAkBkH,KAAA,KAAlB,gBAAAI,EAAA,CAA0B;wBAErC,IAAIE,KAAA,EAAM;0BACRvI,MAAA,CAAO5F,IAAA,GAAOmO,KAAA;0BAEd,IAAIjJ,UAAA,EAAY;4BACdA,UAAA,CAAWU,MAAM;0BACnB;wBACF;sBACF;oBACF,SAAS+B,GAAA,EAAP;sBACA,IAAIyF,MAAA,CAAKlK,MAAA,EAAQ;wBACfhC,OAAA,CAAQ0G,IAAA,CAAK,iCAAiCD,GAAG;sBACnD;oBAEF;kBACF;kBACAnJ,OAAA,EAAS,SAAAA,QAACmJ,GAAA,EAAQ;oBAChBjB,MAAA,CAAOiB,GAAG;kBACZ;gBACF,GACAyF,MAAA,CAAKjK,MACP,EAAE0E,KAAA,CAAM,UAACF,GAAA,EAAQ;kBACf,IAAMyG,WAAA,GAAczG,GAAA,CAAIvG,QAAA,CAAS,EAAEiN,WAAA,CAAY;kBAE/C,IACEzI,MAAA,CAAO5F,IAAA,KACNoO,WAAA,KAAgB,kCACfA,WAAA,KAAgB,0BAClB;oBAKA,OAAO3H,OAAA,CAAQb,MAAM;kBACvB,OAAO;oBACL,OAAOc,MAAA,CAAOiB,GAAG;kBACnB;gBACF,CAAC;cACH,CAAC;cAAA,KAEG1C,SAAA;gBAAA0I,SAAA,CAAArQ,IAAA;gBAAA;cAAA;cACF,IAAIgI,eAAA,EAAiB;gBAGnB;gBAAEO,SAAA,CAAkBmD,MAAA,GAAS,YAAM;kBACjC1D,eAAA,CAAgB2D,KAAA,CAAM;gBACxB;cACF;cAAA,OAAA0E,SAAA,CAAAhQ,MAAA,WAEOuO,SAAA,CAASrG,SAAA,EAAW;gBACzBqD,YAAA,EAAcjE,SAAA;gBACdjE,OAAA,EAAS;cACX,CAAC;YAAA;cAAA,OAAA2M,SAAA,CAAAhQ,MAAA,WAEMkI,SAAA;YAAA;YAAA;cAAA,OAAA8H,SAAA,CAAA7P,IAAA;UAAA;QAAA,GAAAqP,QAAA;MAAA,CAEX;MAAA,SAAAhE,YAAAmF,IAAA;QAAA,OAAApB,aAAA,CAAApR,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAoN,WAAA;IAAA;EAAA;EAAA,OAAAoD,yBAAA;AAAA,GACF"},"metadata":{},"sourceType":"module","externalDependencies":[]}